{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "This tutorial is available as an IPython notebook at [Malaya/example/transformer](https://github.com/huseinzol05/Malaya/tree/master/example/transformer).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the list of dataset we pretrained,\n",
    "\n",
    "Standard Bahasa dataset, \n",
    "\n",
    "1. [Malay-dataset/dumping](https://github.com/huseinzol05/Malay-Dataset/tree/master/dumping).\n",
    "2. [Malay-dataset/pure-text](https://github.com/huseinzol05/Malay-Dataset/tree/master/pure-text).\n",
    "\n",
    "Bahasa social media,\n",
    "\n",
    "1. [Malay-dataset/dumping/instagram](https://github.com/huseinzol05/Malay-Dataset/tree/master/dumping/instagram).\n",
    "2. [Malay-dataset/dumping/twitter](https://github.com/huseinzol05/Malay-Dataset/tree/master/dumping/twitter).\n",
    "\n",
    "Singlish / Manglish,\n",
    "\n",
    "1. [Malay-dataset/dumping/singlish](https://github.com/huseinzol05/Malay-Dataset/tree/master/dumping/singlish-text).\n",
    "2. [Malay-dataset/dumping/singapore-news](https://github.com/huseinzol05/Malay-Dataset/tree/master/dumping/singapore-news).\n",
    "\n",
    "For T5 transformer models, the models trained on MNLI both english and malay languages.\n",
    "\n",
    "**This interface not able us to use it to do custom training**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to download pretrained model for Transformer-Bahasa and use it for custom transfer-learning, you can download it here, https://github.com/huseinzol05/Malaya/tree/master/pretrained-model/, some notebooks to help you get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.8/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "CPU times: user 2.72 s, sys: 2.27 s, total: 4.99 s\n",
      "Wall time: 2.51 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/dev/malaya/malaya/tokenizer.py:214: FutureWarning: Possible nested set at position 3397\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n",
      "/home/husein/dev/malaya/malaya/tokenizer.py:214: FutureWarning: Possible nested set at position 3927\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import malaya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list Transformer HuggingFace available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mesolitica/roberta-base-bahasa-cased': {'Size (MB)': 443},\n",
       " 'mesolitica/roberta-tiny-bahasa-cased': {'Size (MB)': 66.1},\n",
       " 'mesolitica/bert-base-standard-bahasa-cased': {'Size (MB)': 443},\n",
       " 'mesolitica/bert-tiny-standard-bahasa-cased': {'Size (MB)': 66.1},\n",
       " 'mesolitica/roberta-base-standard-bahasa-cased': {'Size (MB)': 443},\n",
       " 'mesolitica/roberta-tiny-standard-bahasa-cased': {'Size (MB)': 66.1},\n",
       " 'mesolitica/electra-base-generator-bahasa-cased': {'Size (MB)': 140},\n",
       " 'mesolitica/electra-small-generator-bahasa-cased': {'Size (MB)': 19.3}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malaya.transformer.available_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = ['Kerajaan galakkan rakyat naik public transport tapi parking kat lrt ada 15. Reserved utk staff rapid je dah berpuluh. Park kereta tepi jalan kang kene saman dgn majlis perbandaran. Kereta pulak senang kene curi. Cctv pun tak ada. Naik grab dah 5-10 ringgit tiap hari. Gampang juga',\n",
    "           'Alaa Tun lek ahhh npe muka masam cmni kn agong kata usaha kerajaan terdahulu sejak selepas merdeka',\n",
    "           \"Orang ramai cakap nurse kerajaan garang. So i tell u this. Most of our local ppl will treat us as hamba abdi and they don't respect us as a nurse\",\n",
    "          'Pemuda mogok lapar desak kerajaan prihatin isu iklim',\n",
    "          'kerajaan perlu kisah isu iklim, pemuda mogok lapar',\n",
    "          'Kerajaan dicadang tubuh jawatankuasa khas tangani isu alam sekitar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load HuggingFace model\n",
    "\n",
    "```python\n",
    "def huggingface(\n",
    "    model: str = 'mesolitica/electra-base-generator-bahasa-cased',\n",
    "    force_check: bool = True,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load transformer model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: str, optional (default='mesolitica/electra-base-generator-bahasa-cased')\n",
    "        Check available models at `malaya.transformer.available_transformer()`.\n",
    "    force_check: bool, optional (default=True)\n",
    "        Force check model one of malaya model.\n",
    "        Set to False if you have your own huggingface model.\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
     ]
    }
   ],
   "source": [
    "model = malaya.transformer.huggingface(model = 'mesolitica/electra-base-generator-bahasa-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have random sentences copied from Twitter, searched using `kerajaan` keyword."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization\n",
    "\n",
    "Change a string or batch of strings to latent space / vectors representation.\n",
    "\n",
    "```python\n",
    "def vectorize(\n",
    "    self,\n",
    "    strings: List[str],\n",
    "    method: str = 'last',\n",
    "    method_token: str = 'first',\n",
    "    t5_head_logits: bool = True,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Vectorize string inputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    strings: List[str]\n",
    "    method: str, optional (default='last')\n",
    "        hidden layers supported. Allowed values:\n",
    "\n",
    "        * ``'last'`` - last layer.\n",
    "        * ``'first'`` - first layer.\n",
    "        * ``'mean'`` - average all layers.\n",
    "\n",
    "        This only applicable for non T5 models.\n",
    "    method_token: str, optional (default='first')\n",
    "        token layers supported. Allowed values:\n",
    "\n",
    "        * ``'last'`` - last token.\n",
    "        * ``'first'`` - first token.\n",
    "        * ``'mean'`` - average all tokens.\n",
    "\n",
    "        usually pretrained models trained on `first` token for classification task.\n",
    "        This only applicable for non T5 models.\n",
    "    t5_head_logits: str, optional (default=True)\n",
    "        if True, will take head logits, else, last token.\n",
    "        This only applicable for T5 models.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result: np.array\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6, 256)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = model.vectorize(strings)\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000001 , 0.72213906, 0.70548344, 0.6682126 , 0.6442658 ,\n",
       "        0.68018407],\n",
       "       [0.72213906, 1.        , 0.62266254, 0.7186686 , 0.69928503,\n",
       "        0.7106037 ],\n",
       "       [0.70548344, 0.62266254, 0.9999999 , 0.6309348 , 0.63519955,\n",
       "        0.62969273],\n",
       "       [0.6682126 , 0.7186686 , 0.6309348 , 1.0000001 , 0.9547028 ,\n",
       "        0.8564713 ],\n",
       "       [0.6442658 , 0.69928503, 0.63519955, 0.9547028 , 1.0000001 ,\n",
       "        0.8234203 ],\n",
       "       [0.68018407, 0.7106037 , 0.62969273, 0.8564713 , 0.8234203 ,\n",
       "        1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def attention(\n",
    "    self,\n",
    "    strings: List[str],\n",
    "    method: str = 'last',\n",
    "    method_head: str = 'mean',\n",
    "    t5_attention: str = 'cross_attentions',\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Get attention string inputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    strings: List[str]\n",
    "    method: str, optional (default='last')\n",
    "        Attention layer supported. Allowed values:\n",
    "\n",
    "        * ``'last'`` - attention from last layer.\n",
    "        * ``'first'`` - attention from first layer.\n",
    "        * ``'mean'`` - average attentions from all layers.\n",
    "    method_head: str, optional (default='mean')\n",
    "        attention head layer supported. Allowed values:\n",
    "\n",
    "        * ``'last'`` - attention from last layer.\n",
    "        * ``'first'`` - attention from first layer.\n",
    "        * ``'mean'`` - average attentions from all layers.\n",
    "    t5_attention: str, optional (default='cross_attentions')\n",
    "        attention type for T5 models. Allowed values:\n",
    "\n",
    "        * ``'cross_attentions'`` - cross attention.\n",
    "        * ``'encoder_attentions'`` - encoder attention.\n",
    "        * ``'decoder_attentions'`` - decoder attention.\n",
    "\n",
    "        This only applicable for T5 models.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : List[List[Tuple[str, float]]]\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can give list of strings or a string to get the attention, in this documentation, I just want to use a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Alaa', 0.05886863),\n",
       "  ('Tun', 0.0612526),\n",
       "  ('lek', 0.06898941),\n",
       "  ('ahhh', 0.064397976),\n",
       "  ('npe', 0.050825186),\n",
       "  ('muka', 0.07244484),\n",
       "  ('masam', 0.053202268),\n",
       "  ('cmni', 0.04823282),\n",
       "  ('kn', 0.058162007),\n",
       "  ('agong', 0.06559846),\n",
       "  ('kata', 0.05514032),\n",
       "  ('usaha', 0.057437427),\n",
       "  ('kerajaan', 0.04105992),\n",
       "  ('terdahulu', 0.044371355),\n",
       "  ('sejak', 0.06925423),\n",
       "  ('selepas', 0.06948459),\n",
       "  ('merdeka', 0.061277937)]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.attention([strings[1]], method = 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Alaa', 0.061838076),\n",
       "  ('Tun', 0.053071998),\n",
       "  ('lek', 0.047781985),\n",
       "  ('ahhh', 0.046944533),\n",
       "  ('npe', 0.052150372),\n",
       "  ('muka', 0.05392791),\n",
       "  ('masam', 0.058074415),\n",
       "  ('cmni', 0.08068735),\n",
       "  ('kn', 0.05034356),\n",
       "  ('agong', 0.054398913),\n",
       "  ('kata', 0.057019003),\n",
       "  ('usaha', 0.058209922),\n",
       "  ('kerajaan', 0.06937862),\n",
       "  ('terdahulu', 0.080670245),\n",
       "  ('sejak', 0.05798509),\n",
       "  ('selepas', 0.06437355),\n",
       "  ('merdeka', 0.05314437)]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.attention([strings[1]], method = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Alaa', 0.048754197),\n",
       "  ('Tun', 0.054038025),\n",
       "  ('lek', 0.053129513),\n",
       "  ('ahhh', 0.057060346),\n",
       "  ('npe', 0.04947073),\n",
       "  ('muka', 0.06097326),\n",
       "  ('masam', 0.057632357),\n",
       "  ('cmni', 0.07236169),\n",
       "  ('kn', 0.052900266),\n",
       "  ('agong', 0.0538029),\n",
       "  ('kata', 0.07015142),\n",
       "  ('usaha', 0.061375342),\n",
       "  ('kerajaan', 0.06380817),\n",
       "  ('terdahulu', 0.063899584),\n",
       "  ('sejak', 0.05665373),\n",
       "  ('selepas', 0.0524459),\n",
       "  ('merdeka', 0.07154254)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.attention([strings[1]], method = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta = malaya.transformer.huggingface(model = 'mesolitica/roberta-base-standard-bahasa-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('Alaa', 0.052424464),\n",
       "  ('Tun', 0.08523697),\n",
       "  ('lek', 0.06813958),\n",
       "  ('ahhh', 0.06153967),\n",
       "  ('npe', 0.06513652),\n",
       "  ('muka', 0.059199475),\n",
       "  ('masam', 0.061626352),\n",
       "  ('cmni', 0.067371994),\n",
       "  ('kn', 0.0662273),\n",
       "  ('agong', 0.052743725),\n",
       "  ('kata', 0.06723866),\n",
       "  ('usaha', 0.04410252),\n",
       "  ('kerajaan', 0.060376044),\n",
       "  ('terdahulu', 0.04183174),\n",
       "  ('sejak', 0.04189242),\n",
       "  ('selepas', 0.039302673),\n",
       "  ('merdeka', 0.06560987)]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta.attention([strings[1]], method = 'last')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
