{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-1000  checkpoint-2500  checkpoint-4000\t   runs\r\n",
      "checkpoint-1500  checkpoint-3000  checkpoint-500\r\n",
      "checkpoint-2000  checkpoint-3500  final_merged_checkpoint\r\n"
     ]
    }
   ],
   "source": [
    "!ls results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe8459dc5fe443f9cb3e99648034d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    './results/checkpoint-4000', device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = {\n",
    "    'description': 'Template used by Malaya.',\n",
    "    'prompt_input': 'Di bawah ialah arahan yang menerangkan tugasan, termasuk dengan input yang menyediakan konteks lanjut. Tulis jawapan yang sesuai dengan arahan tersebut.\\n\\n### Arahan:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Jawapan:\\n',\n",
    "    'prompt_no_input': 'Di bawah ialah arahan yang menerangkan tugasan. Tulis jawapan yang sesuai dengan arahan tersebut.\\n\\n### Arahan:\\n{instruction}\\n\\n### Jawapan:\\n',\n",
    "    'response_split': '### Jawapan:',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  4671,   289,  1450,   801,   474,   284,   801, 24418,  5403,\n",
       "           343,   574,   286,   759,   574, 11052,   260,   688,   294,   273,\n",
       "         29889, 27415,   275,   432,  1450, 21419,   343,   574,  3999, 29884,\n",
       "          1794,   972,  6249, 24418,  5403,  1935,   344,  4187, 29889,    13,\n",
       "            13,  2277, 29937, 25953,  5403, 29901,    13,    13, 26956,   421,\n",
       "         29968, 29965,  1964, 29909,   323,  1001,  1430, 26788,  2190, 29965,\n",
       "           448, 12693,  9919,   273,  1258,   557,  2766,   413,   300,   392,\n",
       "           375,   273,   338, 29884,   409,  2790,  3249,   286,   996,  1249,\n",
       "         11052,   282,   331,  6574,   262,  2431,   638, 23402, 22318,  1848,\n",
       "           313, 15695, 29897,   972,  6249,   413, 15274,   532,   273,  7655,\n",
       "          6948,   332,   639,  1335,  7889, 29889,    13,    13, 29968,   300,\n",
       "          3357,   349,   331,  6191, 10043,  5061,   996,  6249, 29884, 29892,\n",
       "         12929, 29881,  3536,   348,  3423, 29874,   289,  5968,   532, 29892,\n",
       "           338, 29884,  1935,   344,  4187, 13547,   801,   337,  1388,  6025,\n",
       "           413,  1590,  2606,   652, 29890,   574,  7354, 11052, 13159,  9010,\n",
       "          1153,  3459,   271,  2626,   392,   574,   724,   549,  1935, 21312,\n",
       "           481, 17756,  7655,  1717, 29874,   273, 29889,    13,    13, 29908,\n",
       "         29902, 29874,   298,  1384,   284,   801,   260,   513,   557,   273,\n",
       "          2832,   638,  6025,  1935,  2783,   557,  2930, 29882,   557, 13023,\n",
       "          9919,   273,   343,   574,   413,   300,   392,   375,   273,   338,\n",
       "         29884,   443, 29873,  2679,   286,   996,  1249, 11052,   282,  1590,\n",
       "           574, 29895,   574,   972,  6249,   413,   267,   284,   801,   273,\n",
       "         29889,    13,    13, 29908, 29902,  2146,   297, 29875,  5053,   801,\n",
       "           301,  3304,  6025,  5053,   801,   337,  1388,  6869,   348,   409,\n",
       "           546,  2034, 15187,  1335,   297, 29875,   313,  3946,  9919,   273,\n",
       "         29897,  5516,  4861,   972,  6249, 19119,  4812,   525, 29886,  1590,\n",
       "           574, 29895,   574, 29915, 13023,  1648,   286,   996, 11895,   271,\n",
       "         11052,  3999, 29884,  8088,   338, 29884, 13159,  9010,  1153,  3459,\n",
       "           271,   282,   392,   574,   724,   549,   413,  1022,  1114,   349,\n",
       "         29940,  1699, 29466, 20912,   413,   300,  4106,   652, 29882,   431,\n",
       "           686, 29875,  8882,   279,  3536,   713,   282,  1114, 29489,   262,\n",
       "         17580,    13,    13, 29879,   423,  3274,   343,   574,   274,   557,\n",
       "           481,  2331,   284,   338, 29884,  1935,   344,  4187,    13,    13,\n",
       "            13,  2277, 29937,   435,  1450, 21419, 29901,    13]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "paragraph `KUALA TERENGGANU - Kerajaan didakwa ketandusan isu sehingga mengaitkan pemimpin Perikatan Nasional (PN) dengan kenyataan berunsur perkauman.\n",
    "\n",
    "Ketua Pemuda Pas Terengganu, Mohd Harun Esa berkata, isu tersebut telah reda dan kembali dibangkitkan supaya rakyat memandang serong terhadap parti berkenaan.\n",
    "\n",
    "\"Ia hanyalah tindakan politik dan terdesak pihak kerajaan yang ketandusan isu untuk mengaitkan pembangkang dengan kesalahan.\n",
    "\n",
    "\"Isu ini sudah lama dan sudah reda namun seperti mereka ini (kerajaan) masih dengan mentaliti 'pembangkang' kerana menghangatkan sesuatu isu supaya rakyat pandang serong kepada PN,\" katanya ketika dihubungi Sinar Harian pada Isnin.`\n",
    "\n",
    "siapa yang cakap pasal isu tersebut\n",
    "\"\"\"\n",
    "prompt = template['prompt_no_input'].format(instruction=query)\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "input_ids = inputs['input_ids'].to(model.device)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 100\n",
    "temperature = 0.9\n",
    "top_p = 0.95\n",
    "top_k = 50\n",
    "num_beams = 1\n",
    "do_sample = True\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        do_sample=do_sample,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Di bawah ialah arahan yang menerangkan tugasan. Tulis jawapan yang sesuai dengan arahan tersebut.\\n\\n### Arahan:\\n\\nparagraph `KUALA TERENGGANU - Kerajaan didakwa ketandusan isu sehingga mengaitkan pemimpin Perikatan Nasional (PN) dengan kenyataan berunsur perkauman.\\n\\nKetua Pemuda Pas Terengganu, Mohd Harun Esa berkata, isu tersebut telah reda dan kembali dibangkitkan supaya rakyat memandang serong terhadap parti berkenaan.\\n\\n\"Ia hanyalah tindakan politik dan terdesak pihak kerajaan yang ketandusan isu untuk mengaitkan pembangkang dengan kesalahan.\\n\\n\"Isu ini sudah lama dan sudah reda namun seperti mereka ini (kerajaan) masih dengan mentaliti \\'pembangkang\\' kerana menghangatkan sesuatu isu supaya rakyat pandang serong kepada PN,\" katanya ketika dihubungi Sinar Harian pada Isnin.`\\n\\nsiapa yang cakap pasal isu tersebut\\n\\n\\n### Jawapan:\\nTerengganu adalah salah satu negeri di Malaysia yang memiliki minoriti Islam dan minoriti Buddha. Apabila pihak berkuasa dalam negeri mengaitkan pemimpin PN dengan isu berunsur perkauman, maka itu cukup untuk menyedihkan pelbagai orang di Terengganu.\\n\\nMohd Harun Esa'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generation_output.sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_merged_dir = './results/final_merged_checkpoint'\n",
    "model.save_pretrained(output_merged_dir, safe_serialization=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "config.json\t\t\t  model-00002-of-00002.safetensors\n",
      "generation_config.json\t\t  model.safetensors.index.json\n",
      "model-00001-of-00002.safetensors\n"
     ]
    }
   ],
   "source": [
    "!ls {output_merged_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13570aeaecd343b6a6b5e1d3df412cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6918fe3aee4b7e84ef792dc9e4e775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d7dc34adf246f4a58530f6fd8ec9e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub('llama-7b-hf-512-ms-qlora', organization='mesolitica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfbf3cf0196f4568a315e853db9d6023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/llama-7b-hf-512-ms-qlora/commit/4c44042b3046275d86f804216cecb9eee352fa64', commit_message='Upload tokenizer', commit_description='', oid='4c44042b3046275d86f804216cecb9eee352fa64', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('llama-7b-hf-512-ms-qlora', organization='mesolitica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
