{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3.8 uninstall trl -y\n",
    "# !pip3.8 install git+https://github.com/malaysia-ai/trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 09:50:42.941237: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-16 09:50:43.042475: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-16 09:50:43.599210: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-16 09:50:43.599274: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-16 09:50:43.599277: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf', trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "max_seq_length = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1000 shuf-combine-1536.jsonl > example.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl.trainer.utils import ConstantLengthDataset\n",
    "from datasets import load_dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-95db109a2d6ceab2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/husein/.cache/huggingface/datasets/json/default-95db109a2d6ceab2/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c12e303c004cb19fcebe7d0353be83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c244233561a4139af2f19ca861b7adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733c36799a414a51974ce4d58c4003dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/husein/.cache/huggingface/datasets/json/default-95db109a2d6ceab2/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a139f7850b4615b6fff92bf9697612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_prompts = [\n",
    "    'You are an AI assistant',\n",
    "    'Anda adalah pembantu AI',\n",
    "    'Anda adalah pembantu AI yang berguna',\n",
    "    'You are a helpful assistant',\n",
    "    'Anda seorang pembantu yang berguna',\n",
    "]\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(row):\n",
    "    system_prompt = random.choice(system_prompts)\n",
    "    texts = [f'<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n']\n",
    "\n",
    "    if '<bot>:' in row['input'] and row['output'] is None:\n",
    "        inputs, outputs = [], []\n",
    "        splitted = row['input'].split('<bot>:')\n",
    "        for i in range(len(splitted) - 1):\n",
    "            if i == 0:\n",
    "                human = splitted[i].replace('<manusia>:', '')\n",
    "            else:\n",
    "                human = splitted[i].split('<manusia>:')[1]\n",
    "            bot = splitted[i + 1].split('<manusia>:')[0]\n",
    "            inputs.append(human)\n",
    "            outputs.append(bot)\n",
    "    else:\n",
    "        inputs = [row['input']]\n",
    "        outputs = [row['output']]\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        texts.append(f'{input} [/INST] {output} </s><s>[INST] ')\n",
    "    return {'text': ''.join(texts)}\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files='example.jsonl', split=\"train\")\n",
    "dataset = dataset.map(generate_and_tokenize_prompt)\n",
    "dataset = dataset.remove_columns(['prompt_input', 'input', 'output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>>\\nAnda adalah pembantu AI\\n<</SYS>>\\n\\n Saya sentiasa mahu belajar bermain gitar. Di manakah saya harus bermula?\\n [/INST]  Tempat terbaik untuk bermula ialah dengan mempelajari asas teori muzik, seperti skala dan kord. Anda boleh menemui banyak sumber dalam talian yang mengajar konsep ini secara langkah demi langkah. Selain itu, anda mungkin ingin mempertimbangkan untuk mengambil pelajaran daripada pengajar berpengalaman yang boleh membantu anda mengembangkan teknik dan memperhalusi gaya permainan anda. </s><s>[INST] '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     train_dataset=dataset,\n",
    "#     peft_config=peft_config,\n",
    "#     dataset_text_field=\"text\",\n",
    "#     max_seq_length=script_args.max_seq_length,\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=training_arguments,\n",
    "#     packing=script_args.packing,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_text_field = 'text'\n",
    "# use_formatting_func = None\n",
    "# max_seq_len = max_seq_length\n",
    "# def tokenize(element):\n",
    "#     outputs = tokenizer(\n",
    "#         element[dataset_text_field] if not use_formatting_func else formatting_func(element),\n",
    "#         truncation=True,\n",
    "#         padding=False,\n",
    "#         max_length=max_seq_len,\n",
    "#         return_overflowing_tokens=False,\n",
    "#         return_length=False,\n",
    "#     )\n",
    "\n",
    "#     if use_formatting_func and not self._dataset_sanity_checked:\n",
    "#         if not isinstance(formatting_func(element), list):\n",
    "#             raise ValueError(\n",
    "#                 \"The `formatting_func` should return a list of processed strings since it can lead to silent bugs.\"\n",
    "#             )\n",
    "#         else:\n",
    "#             self._dataset_sanity_checked = True\n",
    "\n",
    "#     return {\"input_ids\": outputs[\"input_ids\"], \"attention_mask\": outputs[\"attention_mask\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl.trainer.utils import ConstantLengthDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant = ConstantLengthDataset(\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    dataset_text_field='text',\n",
    "    formatting_func=None,\n",
    "    seq_length=max_seq_length,\n",
    "    infinite=True,\n",
    "    num_of_sequences=1024,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "i = iter(constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [next(i), next(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "b = data_collator(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tinggal di Mongolia zaman moden.\\n </s><s> [INST]  Adakah dia bekerja dengan sesiapa sahaja?\\n [/INST]  Pada tahun 2009, dia dan anak perempuannya Amina menterjemah \"The Diary of a Young Girl oleh Anne Frank\" ke dalam bahasa Mongolia.\\n </s><s> [INST]  Apakah kerja lain yang dia lakukan?\\n [/INST]  Pada tahun yang sama dia menerbitkan koleksi biografi tentang penulis wanita terkenal \"World\\'s Best Women Writers\".\\n </s><s> [INST]  Adakah dia mempunyai karya lain?\\n [/INST]  Shuudertsetseg kemudian beralih kepada penulisan fiksyen sejarah dan pada tahun 2010 menerbitkan \"The Legendary Queen Anu\" (The Legendary Queen Anu) tentang ratu Mongol Dzungar Khanate abad ke-17 yang eponim yang mengetuai tentera dan meninggal dunia pada Pertempuran Zuunmod pada tahun 1696.\\n </s><s> [INST]  Apakah yang dia selidiki?\\n [/INST]  Karya itu menyentuh tema yang terdapat dalam karya terdahulunya, termasuk kepentingan keluarga, pemerkasaan wanita dan identiti nasional.\\n </s><s> [INST]  Berdasarkan perkara di atas, tulis saya artikel tentang Baatarsuren Togtokhbayar, Mongolia, Baatarsuren Togtokhbayar, membincangkan secara terperinci Manusia, manusia dalam dua perenggan.\\n [/INST]  Baatarsuren Togtokhbayar (Mongolia: Baatarsurengin Togtokhbayar; lahir 18 Januari 1971), dikenali dengan nama pena Shuudertsetseg (Mongolia: Shudertsetseg) ialah seorang wartawan, pengarang, pembuat filem dan kemanusiaan Mongolia. Beliau telah menerima banyak anugerah sastera Mongolia termasuk anugerah \\'Buku Pilihan\\' pada tahun 2007 untuk novelnya Coral Bracelet dan pada tahun 2008 untuk CD Shuudertsetseg beliau. Novel 2010 beliau The Legendary Queen Anu telah dinamakan National Book of the Year untuk Kesusasteraan Mongolia. Pada tahun 2012 dia menulis lakon layar untuk, menghasilkan dan mengarahkan versi filem legenda Anu Hatan, yang dikeluarkan di peringkat antarabangsa sebagai \"Puteri Pahlawan\".\\nSelain kerjaya filem dan penulisannya, Shuudertsetseg terkenal dengan usaha kemanusiaannya. Pada tahun 2008, Jawatankuasa Penganjur Amnesty International Mongolia menamakan beliau sebagai \"Aktivis Hak Asasi Manusia Tahun Ini\" atas sumbangan beliau kepada kesaksamaan jantina dan hak asasi manusia. Beliau menganggotai Lembaga Pengarah Antarabangsa Dana Wanita Mongolia \"MONES\" dan merupakan ahli kumpulan wanita Amnesty International. Pada tahun 1996, semasa bekerja sebagai wartawan di akhbar harian People\\'s Right, dia dinamakan \"Wartawan Tahun Ini\" untuk ceritanya mengenai hak kanak-kanak. </s><s> [INST] <s><s> [INST] <<SYS>>\\nYou are an AI assistant\\n<</SYS>>\\n\\n Apa yang berlaku dalam Copa Sudamericana 1991?\\n [/INST]  Perlawanan pertama berakhir dengan keputusan seri 1–1, dan dalam perlawanan timbal balik, bermain di Chile, Pachuca memenangi kejuaraan Copa Sudamericana dengan jaringan 2–1.\\n </s><s> [INST]  Siapakah pemain bintang itu?\\n [/INST]  Gol Pachuca untuk gelaran itu dibuat pada mulanya, oleh simbol mereka Gabriel Caballero dan dalam permainan cemerlang Chitiva memberikan bola kepada Christian \"El Chaco\" Giménez yang menjaringkan \"Golden Goal\" untuk penaklukan terhebat mereka dalam sejarah mereka.\\n </s><s> [INST]  Apa lagi yang penting tentang peristiwa ini?\\n [/INST]  Dengan berbuat demikian, Pachuca menjadi pasukan Mexico dan pasukan CONCACAF pertama memenangi kejuaraan CONMEBOL.\\n </s><s> [INST]  Adakah terdapat sebarang perkara pertama yang lain?\\n [/INST]  Selain itu, Pachuca juga merupakan satu-satunya pasukan di dunia yang memenangi pertandingan kelab dalam Konfederasi di luar pasukannya sendiri.\\n </s><s> [INST]  Merumuskan perkara di atas, berikan saya artikel tentang Club de Fútbol Pachuca, pasukan bola sepak profesional Mexico, Hidalgo, membincangkan secara terperinci Bahagian Primera Mexico, Bahagian Primera dalam satu perenggan.\\n [/INST]  Club de Fútbol Pachuca ialah pasukan bola sepak profesional Mexico yang berpangkalan di Pachuca, Hidalgo, yang bersaing dalam Liga MX. Diasaskan oleh pelombong Cornish dari Camb'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(next(i)['input_ids'].numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
