{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/huseinzol05/malay-dataset/master/parsing/constituency/train.txt\n",
    "# !wget https://raw.githubusercontent.com/huseinzol05/malay-dataset/master/parsing/constituency/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('mesolitica/nanot5-base-malaysian-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 01:03:10.066690: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-20 01:03:10.151429: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-20 01:03:10.609476: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-20 01:03:10.609664: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-20 01:03:10.609668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/husein/.local/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.11.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "/home/husein/.local/lib/python3.8/site-packages/tensorflow_addons/utils/resource_loader.py:78: UserWarning: You are currently using TensorFlow 2.11.0 and trying to load a custom op (custom_ops/seq2seq/_beam_search_ops.so).\n",
      "TensorFlow Addons has compiled its custom ops against TensorFlow 2.6.0, and there are no compatibility guarantees between the two versions. \n",
      "This means that you might get segfaults when loading the custom op, or other kind of low-level errors.\n",
      " If you do, do not file an issue on Github. This is a known limitation.\n",
      "\n",
      "It might help you to fallback to pure Python ops by setting environment variable `TF_ADDONS_PY_OPS=1` or using `tfa.options.disable_custom_kernel()` in your code. To do that, see https://github.com/tensorflow/addons#gpucpu-custom-ops \n",
      "\n",
      "You can also change the TensorFlow version installed on your system. You would need a TensorFlow version equal to or above 2.6.0 and strictly below 2.7.0.\n",
      " Note that nightly versions of TensorFlow, as well as non-pip TensorFlow like `conda install tensorflow` or compiled from source are not supported.\n",
      "\n",
      "The last solution is to find the TensorFlow Addons version that has custom ops compatible with the TensorFlow installed on your system. To do that, refer to the readme: https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "Cannot import beam_search_ops from Tensorflow Addons, ['malaya.jawi_rumi.deep_model', 'malaya.phoneme.deep_model', 'malaya.rumi_jawi.deep_model', 'malaya.stem.deep_model'] will not available to use, make sure Tensorflow Addons version >= 0.12.0\n",
      "check compatible Tensorflow version with Tensorflow Addons at https://github.com/tensorflow/addons/releases\n"
     ]
    }
   ],
   "source": [
    "import pyximport\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from malaya.function import trees_newline as trees\n",
    "pyximport.install(setup_args={\"include_dirs\": np.get_include()})\n",
    "\n",
    "import chart_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Config\n",
    "from malaya.torch_model.t5 import T5Constituency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_TOKEN_MAPPING = {\n",
    "    \"-LRB-\": \"(\",\n",
    "    \"-RRB-\": \")\",\n",
    "    \"-LCB-\": \"{\",\n",
    "    \"-RCB-\": \"}\",\n",
    "    \"-LSB-\": \"[\",\n",
    "    \"-RSB-\": \"]\",\n",
    "    \"``\": '\"',\n",
    "    \"''\": '\"',\n",
    "    \"`\": \"'\",\n",
    "    '«': '\"',\n",
    "    '»': '\"',\n",
    "    '‘': \"'\",\n",
    "    '’': \"'\",\n",
    "    '“': '\"',\n",
    "    '”': '\"',\n",
    "    '„': '\"',\n",
    "    '‹': \"'\",\n",
    "    '›': \"'\",\n",
    "    \"\\u2013\": \"--\", # en dash\n",
    "    \"\\u2014\": \"--\", # em dash\n",
    "    }\n",
    "\n",
    "def process_word(word):\n",
    "    word = word.replace('\\\\/', '/').replace('\\\\*', '*')\n",
    "    # Mid-token punctuation occurs in biomedical text\n",
    "    word = word.replace('-LSB-', '[').replace('-RSB-', ']')\n",
    "    word = word.replace('-LRB-', '(').replace('-RRB-', ')')\n",
    "    if word == \"n't\" and cleaned_words:\n",
    "        cleaned_words[-1] = cleaned_words[-1] + \"n\"\n",
    "        word = \"'t\"\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Vocabulary(object):\n",
    "    def __init__(self):\n",
    "        self.frozen = False\n",
    "        self.values = []\n",
    "        self.indices = {}\n",
    "        self.counts = collections.defaultdict(int)\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.values)\n",
    "\n",
    "    def value(self, index):\n",
    "        assert 0 <= index < len(self.values)\n",
    "        return self.values[index]\n",
    "\n",
    "    def index(self, value):\n",
    "        if not self.frozen:\n",
    "            self.counts[value] += 1\n",
    "\n",
    "        if value in self.indices:\n",
    "            return self.indices[value]\n",
    "\n",
    "        elif not self.frozen:\n",
    "            self.values.append(value)\n",
    "            self.indices[value] = len(self.values) - 1\n",
    "            return self.indices[value]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown value: {}\".format(value))\n",
    "\n",
    "    def index_or_unk(self, value, unk_value):\n",
    "        assert self.frozen\n",
    "        if value in self.indices:\n",
    "            return self.indices[value]\n",
    "        else:\n",
    "            return self.indices[unk_value]\n",
    "\n",
    "    def count(self, value):\n",
    "        return self.counts[value]\n",
    "\n",
    "    def freeze(self):\n",
    "        self.frozen = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_treebank = trees.load_trees('train.txt')\n",
    "train_parse = [tree.convert() for tree in train_treebank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vocab = Vocabulary()\n",
    "label_vocab.index(())\n",
    "\n",
    "tag_vocab = Vocabulary()\n",
    "START = tokenizer.bos_token\n",
    "STOP = tokenizer.eos_token\n",
    "UNK = tokenizer.unk_token\n",
    "TAG_UNK = \"UNK\"\n",
    "tag_vocab.index(START)\n",
    "tag_vocab.index(STOP)\n",
    "tag_vocab.index(UNK)\n",
    "\n",
    "for tree in train_parse:\n",
    "    nodes = [tree]\n",
    "    while nodes:\n",
    "        node = nodes.pop()\n",
    "        if isinstance(node, trees.InternalParseNode):\n",
    "            label_vocab.index(node.label)\n",
    "            nodes.extend(reversed(node.children))\n",
    "        else:\n",
    "            tag_vocab.index(node.tag)\n",
    "            \n",
    "tag_vocab.freeze()\n",
    "label_vocab.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = T5Config.from_pretrained('mesolitica/nanot5-base-malaysian-cased')\n",
    "config.num_labels = label_vocab.size\n",
    "config.num_tags = tag_vocab.size\n",
    "config.tag_loss_scale = 5.0\n",
    "config.label_vocab = {str(k): v for k, v in label_vocab.indices.items()}\n",
    "config.tag_vocab = tag_vocab.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5Constituency(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_parameters = [param for param in model.parameters() if param.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.AdamW(trainable_parameters, lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchIndices:\n",
    "    \"\"\"\n",
    "    Batch indices container class (used to implement packed batches)\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_idxs_np):\n",
    "        self.batch_idxs_np = batch_idxs_np\n",
    "        # Note that the torch copy will be on GPU if use_cuda is set\n",
    "        self.batch_idxs_torch = torch.from_numpy(batch_idxs_np)\n",
    "\n",
    "        self.batch_size = int(1 + np.max(batch_idxs_np))\n",
    "\n",
    "        batch_idxs_np_extra = np.concatenate([[-1], batch_idxs_np, [-1]])\n",
    "        self.boundaries_np = np.nonzero(batch_idxs_np_extra[1:] != batch_idxs_np_extra[:-1])[0]\n",
    "        self.seq_lens_np = self.boundaries_np[1:] - self.boundaries_np[:-1]\n",
    "        assert len(self.seq_lens_np) == self.batch_size\n",
    "        self.max_len = int(np.max(self.boundaries_np[1:] - self.boundaries_np[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(sentences, golds, subbatch_max_tokens=3000):\n",
    "    lens = [\n",
    "        len(tokenizer.tokenize(' '.join([word for (_, word) in sentence]))) + 2\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "\n",
    "    lens = np.asarray(lens, dtype=int)\n",
    "    lens_argsort = np.argsort(lens).tolist()\n",
    "\n",
    "    num_subbatches = 0\n",
    "    subbatch_size = 1\n",
    "    while lens_argsort:\n",
    "        if (subbatch_size == len(lens_argsort)) or (subbatch_size * lens[lens_argsort[subbatch_size]] > subbatch_max_tokens):\n",
    "            yield [sentences[i] for i in lens_argsort[:subbatch_size]], [golds[i] for i in lens_argsort[:subbatch_size]]\n",
    "            lens_argsort = lens_argsort[subbatch_size:]\n",
    "            num_subbatches += 1\n",
    "            subbatch_size = 1\n",
    "        else:\n",
    "            subbatch_size += 1\n",
    "            \n",
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(\n",
    "            sentence + [pad_int] * (max_sentence_len - len(sentence))\n",
    "        )\n",
    "        seq_lens.append(len(sentence))\n",
    "    return padded_seqs, seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "batch_size = 4\n",
    "\n",
    "trainer.zero_grad()\n",
    "batch_loss_value = 0.0\n",
    "batch_trees = train_parse[start_index:start_index + batch_size]\n",
    "batch_sentences = [[(leaf.tag, leaf.word) for leaf in tree.leaves()] for tree in batch_trees]\n",
    "batch_num_tokens = sum(len(sentence) for sentence in batch_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193.48486328125"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for subbatch_sentences, subbatch_trees in split_batch(batch_sentences, batch_trees):\n",
    "    sentences = subbatch_sentences\n",
    "    golds = subbatch_trees\n",
    "    packed_len = sum([(len(sentence) + 2) for sentence in sentences])\n",
    "    i = 0\n",
    "    tag_idxs = np.zeros(packed_len, dtype=int)\n",
    "    batch_idxs = np.zeros(packed_len, dtype=int)\n",
    "    for snum, sentence in enumerate(sentences):\n",
    "        for (tag, word) in [(START, START)] + sentence + [(STOP, STOP)]:\n",
    "            tag_idxs[i] = tag_vocab.index_or_unk(tag, TAG_UNK)\n",
    "            batch_idxs[i] = snum\n",
    "            i += 1\n",
    "    \n",
    "    batch_idxs = BatchIndices(batch_idxs)\n",
    "    emb_idxs_map = {\n",
    "        'tags': tag_idxs,\n",
    "    }\n",
    "    emb_idxs = [\n",
    "        torch.from_numpy(v)\n",
    "        for k, v in emb_idxs_map.items()\n",
    "    ]\n",
    "    gold_tag_idxs = torch.from_numpy(emb_idxs_map['tags'])\n",
    "    all_input_ids = []\n",
    "    all_word_start_mask = []\n",
    "    all_word_end_mask = []\n",
    "\n",
    "    for snum, sentence in enumerate(sentences):\n",
    "\n",
    "        tokens = []\n",
    "        word_start_mask = []\n",
    "        word_end_mask = []\n",
    "        tokens.append(START)\n",
    "        word_start_mask.append(1)\n",
    "        word_end_mask.append(1)\n",
    "\n",
    "        cleaned_words = []\n",
    "        for _, word in sentence:\n",
    "            cleaned_words.append(process_word(word))\n",
    "\n",
    "        for word in cleaned_words:\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            for _ in range(len(word_tokens)):\n",
    "                word_start_mask.append(0)\n",
    "                word_end_mask.append(0)\n",
    "            word_start_mask[len(tokens)] = 1\n",
    "            word_end_mask[-1] = 1\n",
    "            tokens.extend(word_tokens)\n",
    "        tokens.append(STOP)\n",
    "        word_start_mask.append(1)\n",
    "        word_end_mask.append(1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_word_start_mask.append(word_start_mask)\n",
    "        all_word_end_mask.append(word_end_mask)\n",
    "    \n",
    "    padded = tokenizer.pad({\n",
    "        'input_ids': all_input_ids,\n",
    "    }, return_tensors = 'pt')\n",
    "\n",
    "    all_word_start_mask = torch.from_numpy(np.array(pad_sentence_batch(all_word_start_mask, 0)[0]))\n",
    "    all_word_end_mask = torch.from_numpy(np.array(pad_sentence_batch(all_word_end_mask, 0)[0]))\n",
    "    \n",
    "    padded['sentences'] = sentences\n",
    "    padded['batch_idxs'] = batch_idxs\n",
    "    padded['all_word_start_mask'] = all_word_start_mask\n",
    "    padded['all_word_end_mask'] = all_word_end_mask\n",
    "    padded['gold_tag_idxs'] = gold_tag_idxs\n",
    "    padded['golds'] = golds\n",
    "    \n",
    "    loss, tag_loss = model(**padded)\n",
    "    loss = tag_loss / len(subbatch_sentences) + loss / batch_num_tokens\n",
    "    loss_value = float(loss.data.cpu().numpy())\n",
    "    batch_loss_value += loss_value\n",
    "    if loss_value > 0:\n",
    "        loss.backward()\n",
    "        \n",
    "    padded = tokenizer.pad({\n",
    "        'input_ids': all_input_ids,\n",
    "    }, return_tensors = 'pt')\n",
    "    padded['sentences'] = sentences\n",
    "    padded['batch_idxs'] = batch_idxs\n",
    "    padded['all_word_start_mask'] = all_word_start_mask\n",
    "    padded['all_word_end_mask'] = all_word_end_mask\n",
    "    trees, scores = model(**padded)\n",
    "    \n",
    "grad_norm = torch.nn.utils.clip_grad_norm_(trainable_parameters, 1.0)\n",
    "trainer.step()\n",
    "batch_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
