{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/aisingapore/seacorenlp-data/raw/main/id/constituency/train.txt\n",
    "# !wget https://github.com/aisingapore/seacorenlp-data/raw/main/id/constituency/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 12:34:02.360731: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-06 12:34:02.440985: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-06 12:34:02.904010: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-06 12:34:02.904065: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-06 12:34:02.904068: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/husein/dev/malaya/malaya/tokenizer.py:214: FutureWarning: Possible nested set at position 3397\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n",
      "/home/husein/dev/malaya/malaya/tokenizer.py:214: FutureWarning: Possible nested set at position 3927\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from malaya.function.constituency import evaluate, trees_newline as trees\n",
    "from transformers import AutoTokenizer, T5Config\n",
    "from malaya.torch_model.t5 import T5Constituency\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_TOKEN_MAPPING = {\n",
    "    \"-LRB-\": \"(\",\n",
    "    \"-RRB-\": \")\",\n",
    "    \"-LCB-\": \"{\",\n",
    "    \"-RCB-\": \"}\",\n",
    "    \"-LSB-\": \"[\",\n",
    "    \"-RSB-\": \"]\",\n",
    "    \"``\": '\"',\n",
    "    \"''\": '\"',\n",
    "    \"`\": \"'\",\n",
    "    '«': '\"',\n",
    "    '»': '\"',\n",
    "    '‘': \"'\",\n",
    "    '’': \"'\",\n",
    "    '“': '\"',\n",
    "    '”': '\"',\n",
    "    '„': '\"',\n",
    "    '‹': \"'\",\n",
    "    '›': \"'\",\n",
    "    \"\\u2013\": \"--\",\n",
    "    \"\\u2014\": \"--\",\n",
    "    }\n",
    "\n",
    "def process_word(word):\n",
    "    word = word.replace('\\\\/', '/').replace('\\\\*', '*')\n",
    "    word = word.replace('-LSB-', '[').replace('-RSB-', ']')\n",
    "    word = word.replace('-LRB-', '(').replace('-RRB-', ')')\n",
    "    if word == \"n't\" and cleaned_words:\n",
    "        cleaned_words[-1] = cleaned_words[-1] + \"n\"\n",
    "        word = \"'t\"\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Vocabulary(object):\n",
    "    def __init__(self):\n",
    "        self.frozen = False\n",
    "        self.values = []\n",
    "        self.indices = {}\n",
    "        self.counts = collections.defaultdict(int)\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.values)\n",
    "\n",
    "    def value(self, index):\n",
    "        assert 0 <= index < len(self.values)\n",
    "        return self.values[index]\n",
    "\n",
    "    def index(self, value):\n",
    "        if not self.frozen:\n",
    "            self.counts[value] += 1\n",
    "\n",
    "        if value in self.indices:\n",
    "            return self.indices[value]\n",
    "\n",
    "        elif not self.frozen:\n",
    "            self.values.append(value)\n",
    "            self.indices[value] = len(self.values) - 1\n",
    "            return self.indices[value]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown value: {}\".format(value))\n",
    "\n",
    "    def index_or_unk(self, value, unk_value):\n",
    "        assert self.frozen\n",
    "        if value in self.indices:\n",
    "            return self.indices[value]\n",
    "        else:\n",
    "            return self.indices[unk_value]\n",
    "\n",
    "    def count(self, value):\n",
    "        return self.counts[value]\n",
    "\n",
    "    def freeze(self):\n",
    "        self.frozen = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_treebank = trees.load_trees('train.txt')\n",
    "train_parse = [tree.convert() for tree in train_treebank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_treebank = trees.load_trees('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92992c7e4e534221b52d3ed2d09febd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371907904532487f9a508d2a069bbb70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/803k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4516d3f390184aad9fd98ec3abbc0952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('mesolitica/t5-small-standard-bahasa-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vocab = Vocabulary()\n",
    "label_vocab.index(())\n",
    "\n",
    "tag_vocab = Vocabulary()\n",
    "START = '<s>'\n",
    "STOP = '</s>'\n",
    "UNK = tokenizer.unk_token\n",
    "TAG_UNK = \"UNK\"\n",
    "tag_vocab.index(START)\n",
    "tag_vocab.index(STOP)\n",
    "tag_vocab.index(TAG_UNK)\n",
    "\n",
    "for tree in train_parse:\n",
    "    nodes = [tree]\n",
    "    while nodes:\n",
    "        node = nodes.pop()\n",
    "        if isinstance(node, trees.InternalParseNode):\n",
    "            label_vocab.index(node.label)\n",
    "            nodes.extend(reversed(node.children))\n",
    "        else:\n",
    "            tag_vocab.index(node.tag)\n",
    "            \n",
    "tag_vocab.freeze()\n",
    "label_vocab.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74aaf3617e0048e9a2d2fbd500e905b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/667 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = T5Config.from_pretrained('mesolitica/t5-small-standard-bahasa-cased')\n",
    "config.num_labels = label_vocab.size\n",
    "config.num_tags = tag_vocab.size\n",
    "config.tag_loss_scale = 1.0\n",
    "config.label_vocab = {str(k): v for k, v in label_vocab.indices.items()}\n",
    "config.tag_vocab = tag_vocab.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3de57138af436fbfe12c160fed974b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5Constituency were not initialized from the model checkpoint at mesolitica/t5-small-standard-bahasa-cased and are newly initialized: ['f_tag.3.weight', 'f_tag.1.a_2', 'encoder_encoder.attn_3.w_ks2', 'encoder_encoder.ff_4.w_2p.bias', 'encoder_encoder.ff_7.layer_norm.a_2', 'encoder_encoder.ff_5.w_1c.bias', 'encoder_encoder.attn_1.w_qs1', 'encoder_encoder.ff_4.w_1p.bias', 'encoder_encoder.attn_2.layer_norm.b_2', 'encoder_encoder.ff_1.w_2c.weight', 'encoder_encoder.attn_5.w_qs1', 'f_label.1.a_2', 'encoder_encoder.ff_7.w_2c.bias', 'encoder_encoder.ff_2.layer_norm.a_2', 'f_tag.0.bias', 'encoder_encoder.ff_6.w_1p.bias', 'encoder_encoder.ff_1.layer_norm.b_2', 'encoder_encoder.ff_7.layer_norm.b_2', 'encoder_encoder.ff_2.w_2c.weight', 'encoder_encoder.attn_5.w_vs1', 'encoder_encoder.ff_6.layer_norm.a_2', 'encoder_encoder.attn_7.w_qs1', 'encoder_encoder.ff_0.w_2p.bias', 'encoder_encoder.attn_4.w_qs1', 'encoder_encoder.ff_6.layer_norm.b_2', 'encoder_encoder.attn_0.layer_norm.b_2', 'encoder_encoder.ff_1.w_2p.bias', 'encoder_encoder.attn_7.proj2.weight', 'encoder_encoder.attn_4.proj1.weight', 'encoder_encoder.ff_0.w_1p.bias', 'encoder_encoder.attn_5.proj1.weight', 'encoder_encoder.attn_7.w_ks2', 'encoder_encoder.attn_7.w_ks1', 'encoder_encoder.attn_2.w_qs1', 'encoder_encoder.attn_0.w_ks2', 'encoder_encoder.ff_1.w_2c.bias', 'encoder_encoder.ff_5.w_2c.bias', 'encoder_encoder.ff_1.w_1p.weight', 'f_tag.3.bias', 'encoder_encoder.ff_3.w_2c.weight', 'encoder_encoder.attn_3.proj1.weight', 'encoder_encoder.ff_5.layer_norm.b_2', 'encoder_encoder.attn_6.w_qs2', 'encoder_encoder.ff_0.w_1c.weight', 'encoder_encoder.attn_5.layer_norm.a_2', 'encoder_encoder.attn_3.layer_norm.b_2', 'encoder_encoder.attn_2.w_ks2', 'encoder_encoder.attn_6.w_ks1', 'f_label.3.bias', 'encoder_encoder.ff_7.w_1c.bias', 'encoder_encoder.ff_5.w_1c.weight', 'encoder_encoder.ff_1.layer_norm.a_2', 'encoder_encoder.ff_4.layer_norm.b_2', 'encoder_encoder.ff_6.w_1c.weight', 'encoder_encoder.ff_3.layer_norm.b_2', 'encoder_encoder.ff_4.w_1p.weight', 'encoder_encoder.attn_1.layer_norm.b_2', 'encoder_encoder.ff_6.w_2c.bias', 'encoder_encoder.attn_7.layer_norm.b_2', 'encoder_encoder.ff_0.layer_norm.a_2', 'encoder_encoder.ff_4.w_1c.weight', 'encoder_encoder.attn_5.proj2.weight', 'encoder_encoder.attn_1.proj2.weight', 'encoder_encoder.ff_3.w_2p.weight', 'encoder_encoder.ff_4.w_2c.weight', 'encoder_encoder.ff_6.w_2p.bias', 'encoder_encoder.attn_6.layer_norm.a_2', 'encoder_encoder.attn_7.w_vs1', 'encoder_encoder.attn_5.layer_norm.b_2', 'encoder_encoder.attn_6.proj1.weight', 'encoder_encoder.ff_2.w_1p.bias', 'encoder_encoder.attn_2.w_vs1', 'encoder_encoder.ff_3.w_2c.bias', 'encoder_encoder.ff_3.w_2p.bias', 'encoder_encoder.attn_0.proj2.weight', 'encoder_encoder.ff_6.w_2c.weight', 'encoder_encoder.attn_3.w_vs2', 'f_label.1.b_2', 'encoder_encoder.attn_4.w_qs2', 'encoder_encoder.ff_7.w_2p.weight', 'encoder_encoder.ff_1.w_1c.weight', 'encoder_encoder.attn_2.proj2.weight', 'encoder_encoder.attn_1.w_ks2', 'encoder_encoder.ff_0.w_2c.weight', 'encoder_encoder.ff_4.layer_norm.a_2', 'encoder_encoder.attn_0.w_qs2', 'encoder_encoder.ff_4.w_2c.bias', 'f_tag.1.b_2', 'encoder_encoder.attn_3.w_vs1', 'encoder_encoder.attn_1.w_qs2', 'encoder_encoder.ff_7.w_1c.weight', 'encoder_encoder.attn_4.w_ks2', 'encoder_encoder.ff_5.layer_norm.a_2', 'encoder_encoder.ff_2.w_1c.weight', 'encoder_encoder.attn_5.w_ks1', 'encoder_encoder.attn_7.w_vs2', 'encoder_encoder.ff_7.w_1p.bias', 'encoder_encoder.ff_7.w_1p.weight', 'encoder_encoder.attn_1.w_vs2', 'encoder_encoder.attn_7.proj1.weight', 'encoder_encoder.attn_1.layer_norm.a_2', 'embedding.layer_norm.b_2', 'encoder_encoder.attn_6.layer_norm.b_2', 'encoder_encoder.ff_2.layer_norm.b_2', 'encoder_encoder.attn_6.w_vs1', 'encoder_encoder.ff_6.w_1c.bias', 'encoder_encoder.attn_2.layer_norm.a_2', 'encoder_encoder.ff_5.w_1p.bias', 'encoder_encoder.attn_0.w_vs1', 'f_label.0.weight', 'encoder_encoder.ff_2.w_2p.weight', 'encoder_encoder.ff_5.w_2p.bias', 'encoder_encoder.attn_4.layer_norm.a_2', 'encoder_encoder.ff_2.w_1p.weight', 'encoder_encoder.ff_3.w_1c.bias', 'f_label.3.weight', 'encoder_encoder.attn_1.w_vs1', 'encoder_encoder.ff_2.w_1c.bias', 'project_bert.weight', 'encoder_encoder.ff_5.w_1p.weight', 'encoder_encoder.ff_3.w_1p.bias', 'encoder_encoder.ff_4.w_1c.bias', 'encoder_encoder.attn_4.w_vs1', 'encoder_encoder.ff_2.w_2p.bias', 'encoder_encoder.attn_2.proj1.weight', 'encoder_encoder.ff_5.w_2c.weight', 'encoder_encoder.ff_1.w_2p.weight', 'encoder_encoder.ff_0.layer_norm.b_2', 'f_label.0.bias', 'encoder_encoder.attn_1.proj1.weight', 'encoder_encoder.ff_3.layer_norm.a_2', 'encoder_encoder.attn_2.w_qs2', 'encoder_encoder.ff_7.w_2c.weight', 'encoder_encoder.ff_0.w_1c.bias', 'encoder_encoder.ff_0.w_2p.weight', 'f_tag.0.weight', 'embedding.layer_norm.a_2', 'encoder_encoder.attn_2.w_ks1', 'encoder_encoder.attn_4.w_vs2', 'encoder_encoder.attn_6.w_vs2', 'encoder_encoder.attn_2.w_vs2', 'encoder_encoder.attn_1.w_ks1', 'encoder_encoder.attn_0.w_vs2', 'encoder_encoder.attn_5.w_qs2', 'encoder_encoder.ff_6.w_2p.weight', 'encoder_encoder.attn_4.w_ks1', 'encoder_encoder.attn_0.w_qs1', 'encoder_encoder.attn_6.proj2.weight', 'encoder_encoder.attn_4.proj2.weight', 'encoder_encoder.attn_3.layer_norm.a_2', 'encoder_encoder.ff_3.w_1c.weight', 'encoder_encoder.attn_0.layer_norm.a_2', 'encoder_encoder.attn_0.w_ks1', 'encoder_encoder.ff_2.w_2c.bias', 'encoder_encoder.ff_4.w_2p.weight', 'encoder_encoder.attn_7.layer_norm.a_2', 'encoder_encoder.attn_6.w_qs1', 'encoder_encoder.ff_7.w_2p.bias', 'encoder_encoder.ff_3.w_1p.weight', 'encoder_encoder.attn_5.w_ks2', 'encoder_encoder.attn_5.w_vs2', 'encoder_encoder.attn_7.w_qs2', 'embedding.position_table', 'encoder_encoder.ff_6.w_1p.weight', 'encoder_encoder.attn_6.w_ks2', 'encoder_encoder.attn_4.layer_norm.b_2', 'encoder_encoder.attn_0.proj1.weight', 'encoder_encoder.attn_3.w_ks1', 'encoder_encoder.attn_3.proj2.weight', 'encoder_encoder.attn_3.w_qs2', 'encoder_encoder.ff_1.w_1p.bias', 'encoder_encoder.attn_3.w_qs1', 'encoder_encoder.ff_1.w_1c.bias', 'encoder_encoder.ff_0.w_2c.bias', 'encoder_encoder.ff_0.w_1p.weight', 'encoder_encoder.ff_5.w_2p.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = T5Constituency.from_pretrained('mesolitica/t5-small-standard-bahasa-cased', config = config)\n",
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_parameters = [param for param in model.parameters() if param.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.AdamW(trainable_parameters, lr = 2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchIndices:\n",
    "    def __init__(self, batch_idxs_np):\n",
    "        self.batch_idxs_np = batch_idxs_np\n",
    "        self.batch_idxs_torch = torch.from_numpy(batch_idxs_np)\n",
    "        self.batch_size = int(1 + np.max(batch_idxs_np))\n",
    "\n",
    "        batch_idxs_np_extra = np.concatenate([[-1], batch_idxs_np, [-1]])\n",
    "        self.boundaries_np = np.nonzero(batch_idxs_np_extra[1:] != batch_idxs_np_extra[:-1])[0]\n",
    "        self.seq_lens_np = self.boundaries_np[1:] - self.boundaries_np[:-1]\n",
    "        assert len(self.seq_lens_np) == self.batch_size\n",
    "        self.max_len = int(np.max(self.boundaries_np[1:] - self.boundaries_np[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(sentences, golds, subbatch_max_tokens=3000):\n",
    "    lens = [\n",
    "        len(tokenizer.tokenize(' '.join([word for (_, word) in sentence]))) + 2\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "\n",
    "    lens = np.asarray(lens, dtype=int)\n",
    "    lens_argsort = np.argsort(lens).tolist()\n",
    "\n",
    "    num_subbatches = 0\n",
    "    subbatch_size = 1\n",
    "    while lens_argsort:\n",
    "        if (subbatch_size == len(lens_argsort)) or (subbatch_size * lens[lens_argsort[subbatch_size]] > subbatch_max_tokens):\n",
    "            yield [sentences[i] for i in lens_argsort[:subbatch_size]], [golds[i] for i in lens_argsort[:subbatch_size]]\n",
    "            lens_argsort = lens_argsort[subbatch_size:]\n",
    "            num_subbatches += 1\n",
    "            subbatch_size = 1\n",
    "        else:\n",
    "            subbatch_size += 1\n",
    "            \n",
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(\n",
    "            sentence + [pad_int] * (max_sentence_len - len(sentence))\n",
    "        )\n",
    "        seq_lens.append(len(sentence))\n",
    "    return padded_seqs, seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(sentences, golds = None):\n",
    "    \n",
    "    all_input_ids = []\n",
    "    all_word_start_mask = []\n",
    "    all_word_end_mask = []\n",
    "\n",
    "    for snum, sentence in enumerate(sentences):\n",
    "\n",
    "        tokens = []\n",
    "        word_start_mask = []\n",
    "        word_end_mask = []\n",
    "        tokens.append(START)\n",
    "        word_start_mask.append(1)\n",
    "        word_end_mask.append(1)\n",
    "\n",
    "        cleaned_words = []\n",
    "        for _, word in sentence:\n",
    "            cleaned_words.append(process_word(word))\n",
    "\n",
    "        for word in cleaned_words:\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            for _ in range(len(word_tokens)):\n",
    "                word_start_mask.append(0)\n",
    "                word_end_mask.append(0)\n",
    "            word_start_mask[len(tokens)] = 1\n",
    "            word_end_mask[-1] = 1\n",
    "            tokens.extend(word_tokens)\n",
    "        tokens.append(STOP)\n",
    "        word_start_mask.append(1)\n",
    "        word_end_mask.append(1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_word_start_mask.append(word_start_mask)\n",
    "        all_word_end_mask.append(word_end_mask)\n",
    "    \n",
    "    padded = tokenizer.pad({\n",
    "        'input_ids': all_input_ids,\n",
    "    }, return_tensors = 'pt')\n",
    "\n",
    "    all_word_start_mask = torch.from_numpy(np.array(pad_sentence_batch(all_word_start_mask, 0)[0]))\n",
    "    all_word_end_mask = torch.from_numpy(np.array(pad_sentence_batch(all_word_end_mask, 0)[0]))\n",
    "    \n",
    "    padded['sentences'] = sentences\n",
    "    padded['all_word_start_mask'] = all_word_start_mask\n",
    "    padded['all_word_end_mask'] = all_word_end_mask\n",
    "    \n",
    "    packed_len = sum([(len(sentence) + 2) for sentence in sentences])\n",
    "    i = 0\n",
    "    tag_idxs = np.zeros(packed_len, dtype=int)\n",
    "    batch_idxs = np.zeros(packed_len, dtype=int)\n",
    "    for snum, sentence in enumerate(sentences):\n",
    "        for (tag, word) in [(START, START)] + sentence + [(STOP, STOP)]:\n",
    "            if golds is not None:\n",
    "                tag_idxs[i] = tag_vocab.index_or_unk(tag, TAG_UNK)\n",
    "            else:\n",
    "                tag_idxs[i] = 0\n",
    "            batch_idxs[i] = snum\n",
    "            i += 1\n",
    "    \n",
    "    batch_idxs = BatchIndices(batch_idxs)\n",
    "    padded['batch_idxs'] = batch_idxs\n",
    "    tag_idxs = torch.from_numpy(tag_idxs)\n",
    "    padded['tag_idxs'] = tag_idxs\n",
    "    \n",
    "    if golds is not None:\n",
    "        gold_tag_idxs = tag_idxs\n",
    "        padded['gold_tag_idxs'] = gold_tag_idxs\n",
    "        padded['golds'] = golds\n",
    "        \n",
    "        \n",
    "    if torch.cuda.is_available():\n",
    "        for k in padded.keys():\n",
    "            if isinstance(padded[k], torch.Tensor):\n",
    "                padded[k] = padded[k].cuda()\n",
    "    \n",
    "        padded['batch_idxs'].batch_idxs_torch = padded['batch_idxs'].batch_idxs_torch.cuda()\n",
    "    \n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(300.6927, device='cuda:0', grad_fn=<AddBackward0>), tensor(280.9204, device='cuda:0', grad_fn=<MulBackward0>))\n"
     ]
    }
   ],
   "source": [
    "start_index = 0\n",
    "batch_size = 4\n",
    "batch_trees = train_parse[start_index:start_index + batch_size]\n",
    "batch_sentences = [[(leaf.tag, leaf.word) for leaf in tree.leaves()] for tree in batch_trees]\n",
    "for subbatch_sentences, subbatch_trees in split_batch(batch_sentences, batch_trees):\n",
    "    print(model(**process(subbatch_sentences, subbatch_trees)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model(**process(subbatch_sentences)), model(**process(subbatch_sentences, subbatch_trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:49<00:00, 10.18it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 7.743378249883651, dev_fscore: (Recall=69.78, Precision=73.57, FScore=71.63, CompleteMatch=7.50, TaggingAccuracy=93.89)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:45<00:00, 10.90it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 3.680590521335602, dev_fscore: (Recall=75.00, Precision=75.88, FScore=75.44, CompleteMatch=12.30, TaggingAccuracy=94.42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:45<00:00, 10.98it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, loss: 2.4897709555625918, dev_fscore: (Recall=75.35, Precision=78.94, FScore=77.10, CompleteMatch=13.60, TaggingAccuracy=94.68)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:47<00:00, 10.55it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, loss: 1.8179046350121497, dev_fscore: (Recall=76.88, Precision=79.49, FScore=78.17, CompleteMatch=16.50, TaggingAccuracy=95.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:45<00:00, 10.96it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, loss: 1.3630381355285643, dev_fscore: (Recall=78.06, Precision=79.38, FScore=78.71, CompleteMatch=17.70, TaggingAccuracy=94.79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:45<00:00, 10.95it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, loss: 1.146106514930725, dev_fscore: (Recall=79.12, Precision=80.29, FScore=79.70, CompleteMatch=19.00, TaggingAccuracy=94.98)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:45<00:00, 10.94it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, loss: 0.9282996150851249, dev_fscore: (Recall=80.48, Precision=80.38, FScore=80.43, CompleteMatch=19.90, TaggingAccuracy=94.89)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:45<00:00, 10.91it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, loss: 0.8417778767347336, dev_fscore: (Recall=82.12, Precision=80.10, FScore=81.10, CompleteMatch=20.00, TaggingAccuracy=94.85)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:45<00:00, 10.96it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, loss: 0.7515054059028625, dev_fscore: (Recall=81.64, Precision=80.08, FScore=80.85, CompleteMatch=20.30, TaggingAccuracy=94.86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:45<00:00, 10.96it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, loss: 0.6541142633259296, dev_fscore: (Recall=81.56, Precision=80.82, FScore=81.19, CompleteMatch=20.30, TaggingAccuracy=94.76)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:45<00:00, 10.95it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss: 0.6230107491910457, dev_fscore: (Recall=81.99, Precision=80.74, FScore=81.36, CompleteMatch=20.30, TaggingAccuracy=94.85)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.69it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11, loss: 0.5766208143681287, dev_fscore: (Recall=81.39, Precision=81.05, FScore=81.22, CompleteMatch=20.60, TaggingAccuracy=94.94)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.82it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, loss: 0.5240447754412889, dev_fscore: (Recall=82.35, Precision=81.29, FScore=81.82, CompleteMatch=21.40, TaggingAccuracy=94.86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:45<00:00, 10.88it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13, loss: 0.5033269971311093, dev_fscore: (Recall=82.55, Precision=81.00, FScore=81.77, CompleteMatch=21.70, TaggingAccuracy=94.84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.80it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, loss: 0.46285473348200323, dev_fscore: (Recall=82.52, Precision=81.03, FScore=81.77, CompleteMatch=22.90, TaggingAccuracy=94.92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.74it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, loss: 0.43112155309319494, dev_fscore: (Recall=81.68, Precision=81.94, FScore=81.81, CompleteMatch=21.30, TaggingAccuracy=95.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:47<00:00, 10.61it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16, loss: 0.4138983658850193, dev_fscore: (Recall=82.31, Precision=82.01, FScore=82.16, CompleteMatch=24.00, TaggingAccuracy=94.93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.65it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17, loss: 0.36190101253986356, dev_fscore: (Recall=82.29, Precision=81.79, FScore=82.04, CompleteMatch=21.50, TaggingAccuracy=94.83)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.87it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18, loss: 0.33861574751883744, dev_fscore: (Recall=82.53, Precision=81.68, FScore=82.11, CompleteMatch=22.40, TaggingAccuracy=95.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.82it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19, loss: 0.3314727769270539, dev_fscore: (Recall=82.09, Precision=82.19, FScore=82.14, CompleteMatch=21.80, TaggingAccuracy=94.81)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.72it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss: 0.3376833411976695, dev_fscore: (Recall=81.72, Precision=82.26, FScore=81.99, CompleteMatch=21.70, TaggingAccuracy=94.79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:45<00:00, 10.88it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21, loss: 0.3095257109254599, dev_fscore: (Recall=82.18, Precision=82.42, FScore=82.30, CompleteMatch=23.00, TaggingAccuracy=95.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.77it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22, loss: 0.30419243048131467, dev_fscore: (Recall=82.59, Precision=81.32, FScore=81.95, CompleteMatch=21.90, TaggingAccuracy=95.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.82it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23, loss: 0.2654794286713004, dev_fscore: (Recall=82.46, Precision=81.77, FScore=82.11, CompleteMatch=23.60, TaggingAccuracy=94.92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.77it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24, loss: 0.2652436782196164, dev_fscore: (Recall=81.62, Precision=83.32, FScore=82.46, CompleteMatch=22.40, TaggingAccuracy=94.95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.86it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25, loss: 0.26486815672367814, dev_fscore: (Recall=81.70, Precision=82.50, FScore=82.10, CompleteMatch=22.60, TaggingAccuracy=94.91)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:45<00:00, 10.87it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26, loss: 0.2550786470863968, dev_fscore: (Recall=81.19, Precision=82.79, FScore=81.98, CompleteMatch=22.40, TaggingAccuracy=95.09)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.78it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27, loss: 0.2116141620799899, dev_fscore: (Recall=81.62, Precision=82.33, FScore=81.98, CompleteMatch=21.10, TaggingAccuracy=94.80)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.77it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28, loss: 0.2125208693742752, dev_fscore: (Recall=81.75, Precision=82.02, FScore=81.88, CompleteMatch=20.70, TaggingAccuracy=94.93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.84it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29, loss: 0.21173147058114408, dev_fscore: (Recall=82.16, Precision=81.92, FScore=82.04, CompleteMatch=22.20, TaggingAccuracy=95.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.78it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30, loss: 0.21903378688916564, dev_fscore: (Recall=82.03, Precision=82.18, FScore=82.11, CompleteMatch=22.40, TaggingAccuracy=95.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.77it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31, loss: 0.19343078238144518, dev_fscore: (Recall=81.69, Precision=82.68, FScore=82.18, CompleteMatch=22.60, TaggingAccuracy=94.96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.85it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32, loss: 0.2054460514318198, dev_fscore: (Recall=81.78, Precision=82.78, FScore=82.28, CompleteMatch=24.00, TaggingAccuracy=95.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.71it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33, loss: 0.19851240930706263, dev_fscore: (Recall=82.08, Precision=82.48, FScore=82.28, CompleteMatch=22.50, TaggingAccuracy=95.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:46<00:00, 10.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34, loss: 0.17575171417556704, dev_fscore: (Recall=81.71, Precision=82.67, FScore=82.18, CompleteMatch=23.10, TaggingAccuracy=95.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "epoch = 100\n",
    "\n",
    "best_dev_fscore = -np.inf\n",
    "patient = 10\n",
    "current_patient = 0\n",
    "\n",
    "for e in range(epoch):\n",
    "    pbar = tqdm(range(0, len(train_parse), batch_size))\n",
    "    losses = []\n",
    "    for start_index in pbar:\n",
    "        trainer.zero_grad()\n",
    "        batch_loss_value = 0.0\n",
    "        batch_trees = train_parse[start_index:start_index + batch_size]\n",
    "        batch_sentences = [[(leaf.tag, leaf.word) for leaf in tree.leaves()] for tree in batch_trees]\n",
    "        batch_num_tokens = sum(len(sentence) for sentence in batch_sentences)\n",
    "\n",
    "        for subbatch_sentences, subbatch_trees in split_batch(batch_sentences, batch_trees):\n",
    "            loss, tag_loss =  model(**process(subbatch_sentences, subbatch_trees))\n",
    "            loss = tag_loss / len(subbatch_sentences) + loss / batch_num_tokens\n",
    "            loss_value = float(loss.data.cpu().numpy())\n",
    "            batch_loss_value += loss_value\n",
    "            if loss_value > 0:\n",
    "                loss.backward()\n",
    "        \n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(trainable_parameters, 5.0)\n",
    "        trainer.step()\n",
    "        losses.append(batch_loss_value)\n",
    "        \n",
    "    dev_predicted = []\n",
    "    for dev_start_index in range(0, len(dev_treebank), batch_size):\n",
    "        subbatch_trees = dev_treebank[dev_start_index:dev_start_index+batch_size]\n",
    "        subbatch_sentences = [[(leaf.tag, leaf.word) for leaf in tree.leaves()] for tree in subbatch_trees]\n",
    "        predicted, _ =  model(**process(subbatch_sentences))\n",
    "        dev_predicted.extend([p.convert() for p in predicted])\n",
    "    \n",
    "    dev_fscore = evaluate.evalb('deprecated/EVALB', dev_treebank, dev_predicted)\n",
    "    \n",
    "    print(f'epoch: {e}, loss: {np.mean(losses)}, dev_fscore: {dev_fscore}')\n",
    "    \n",
    "    if dev_fscore.fscore >= best_dev_fscore:\n",
    "        best_dev_fscore = dev_fscore.fscore\n",
    "        current_patient = 0\n",
    "        model.save_pretrained('base')\n",
    "    else:\n",
    "        current_patient += 1\n",
    "    \n",
    "    if current_patient >= patient:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.46"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_dev_fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = T5Constituency.from_pretrained('./base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480a8b4ca89d47c28c1eb88c604f75ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/247M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/constituency-parsing-t5-small-standard-bahasa-cased/commit/6e5992ef042814a673d44df53222c598aad6f766', commit_message='Upload T5Constituency', commit_description='', oid='6e5992ef042814a673d44df53222c598aad6f766', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_.push_to_hub('mesolitica/constituency-parsing-t5-small-standard-bahasa-cased', safe_serialization = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75edd14ecbe249b8849c247414b8b7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/803k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/constituency-parsing-t5-small-standard-bahasa-cased/commit/e1e0c23a17cf24d67fca055959c028af6de6d5f9', commit_message='Upload tokenizer', commit_description='', oid='e1e0c23a17cf24d67fca055959c028af6de6d5f9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('mesolitica/constituency-parsing-t5-small-standard-bahasa-cased', safe_serialization = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
