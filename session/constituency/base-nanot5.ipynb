{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/aisingapore/seacorenlp-data/raw/main/id/constituency/train.txt\n",
    "# !wget https://github.com/aisingapore/seacorenlp-data/raw/main/id/constituency/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 10:10:36.031542: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-21 10:10:36.108454: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-21 10:10:36.554157: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-21 10:10:36.554188: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-21 10:10:36.554191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/husein/dev/malaya/malaya/tokenizer.py:214: FutureWarning: Possible nested set at position 3397\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n",
      "/home/husein/dev/malaya/malaya/tokenizer.py:214: FutureWarning: Possible nested set at position 3927\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from malaya.function.constituency import evaluate, trees_newline as trees\n",
    "from transformers import AutoTokenizer, T5Config\n",
    "from malaya.torch_model.t5 import T5Constituency\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_TOKEN_MAPPING = {\n",
    "    \"-LRB-\": \"(\",\n",
    "    \"-RRB-\": \")\",\n",
    "    \"-LCB-\": \"{\",\n",
    "    \"-RCB-\": \"}\",\n",
    "    \"-LSB-\": \"[\",\n",
    "    \"-RSB-\": \"]\",\n",
    "    \"``\": '\"',\n",
    "    \"''\": '\"',\n",
    "    \"`\": \"'\",\n",
    "    '«': '\"',\n",
    "    '»': '\"',\n",
    "    '‘': \"'\",\n",
    "    '’': \"'\",\n",
    "    '“': '\"',\n",
    "    '”': '\"',\n",
    "    '„': '\"',\n",
    "    '‹': \"'\",\n",
    "    '›': \"'\",\n",
    "    \"\\u2013\": \"--\",\n",
    "    \"\\u2014\": \"--\",\n",
    "    }\n",
    "\n",
    "def process_word(word):\n",
    "    word = word.replace('\\\\/', '/').replace('\\\\*', '*')\n",
    "    word = word.replace('-LSB-', '[').replace('-RSB-', ']')\n",
    "    word = word.replace('-LRB-', '(').replace('-RRB-', ')')\n",
    "    if word == \"n't\" and cleaned_words:\n",
    "        cleaned_words[-1] = cleaned_words[-1] + \"n\"\n",
    "        word = \"'t\"\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Vocabulary(object):\n",
    "    def __init__(self):\n",
    "        self.frozen = False\n",
    "        self.values = []\n",
    "        self.indices = {}\n",
    "        self.counts = collections.defaultdict(int)\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.values)\n",
    "\n",
    "    def value(self, index):\n",
    "        assert 0 <= index < len(self.values)\n",
    "        return self.values[index]\n",
    "\n",
    "    def index(self, value):\n",
    "        if not self.frozen:\n",
    "            self.counts[value] += 1\n",
    "\n",
    "        if value in self.indices:\n",
    "            return self.indices[value]\n",
    "\n",
    "        elif not self.frozen:\n",
    "            self.values.append(value)\n",
    "            self.indices[value] = len(self.values) - 1\n",
    "            return self.indices[value]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown value: {}\".format(value))\n",
    "\n",
    "    def index_or_unk(self, value, unk_value):\n",
    "        assert self.frozen\n",
    "        if value in self.indices:\n",
    "            return self.indices[value]\n",
    "        else:\n",
    "            return self.indices[unk_value]\n",
    "\n",
    "    def count(self, value):\n",
    "        return self.counts[value]\n",
    "\n",
    "    def freeze(self):\n",
    "        self.frozen = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_treebank = trees.load_trees('train.txt')\n",
    "train_parse = [tree.convert() for tree in train_treebank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_treebank = trees.load_trees('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('mesolitica/nanot5-base-malaysian-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vocab = Vocabulary()\n",
    "label_vocab.index(())\n",
    "\n",
    "tag_vocab = Vocabulary()\n",
    "START = '<s>'\n",
    "STOP = '</s>'\n",
    "UNK = tokenizer.unk_token\n",
    "TAG_UNK = \"UNK\"\n",
    "tag_vocab.index(START)\n",
    "tag_vocab.index(STOP)\n",
    "tag_vocab.index(TAG_UNK)\n",
    "\n",
    "for tree in train_parse:\n",
    "    nodes = [tree]\n",
    "    while nodes:\n",
    "        node = nodes.pop()\n",
    "        if isinstance(node, trees.InternalParseNode):\n",
    "            label_vocab.index(node.label)\n",
    "            nodes.extend(reversed(node.children))\n",
    "        else:\n",
    "            tag_vocab.index(node.tag)\n",
    "            \n",
    "tag_vocab.freeze()\n",
    "label_vocab.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = T5Config.from_pretrained('mesolitica/nanot5-base-malaysian-cased')\n",
    "config.num_labels = label_vocab.size\n",
    "config.num_tags = tag_vocab.size\n",
    "config.tag_loss_scale = 1.0\n",
    "config.label_vocab = {str(k): v for k, v in label_vocab.indices.items()}\n",
    "config.tag_vocab = tag_vocab.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5Constituency were not initialized from the model checkpoint at mesolitica/nanot5-base-malaysian-cased and are newly initialized: ['encoder_encoder.attn_1.w_qs1', 'encoder_encoder.ff_1.w_2c.bias', 'encoder_encoder.attn_5.proj1.weight', 'encoder_encoder.ff_5.layer_norm.b_2', 'encoder_encoder.attn_7.w_qs2', 'encoder_encoder.ff_6.w_2p.weight', 'encoder_encoder.attn_2.w_vs1', 'encoder_encoder.attn_2.proj2.weight', 'encoder_encoder.ff_4.w_2c.bias', 'f_label.3.weight', 'encoder_encoder.ff_7.w_2p.bias', 'f_tag.0.bias', 'encoder_encoder.ff_1.w_1p.bias', 'encoder_encoder.ff_6.layer_norm.b_2', 'encoder_encoder.ff_0.w_2p.weight', 'encoder_encoder.ff_7.w_1c.bias', 'encoder_encoder.attn_0.w_vs1', 'encoder_encoder.attn_1.layer_norm.b_2', 'embedding.position_table', 'encoder_encoder.attn_5.w_vs1', 'encoder_encoder.ff_0.w_2p.bias', 'encoder_encoder.attn_4.layer_norm.a_2', 'encoder_encoder.ff_7.w_2p.weight', 'encoder_encoder.ff_0.w_2c.bias', 'encoder_encoder.attn_2.layer_norm.a_2', 'encoder_encoder.attn_3.w_vs2', 'encoder_encoder.attn_3.layer_norm.b_2', 'encoder_encoder.ff_6.layer_norm.a_2', 'encoder_encoder.ff_1.w_1c.bias', 'encoder_encoder.ff_1.w_1p.weight', 'encoder_encoder.attn_6.w_vs2', 'encoder_encoder.attn_7.proj2.weight', 'encoder_encoder.attn_0.w_ks1', 'encoder_encoder.ff_5.w_1c.weight', 'encoder_encoder.attn_5.layer_norm.b_2', 'encoder_encoder.ff_6.w_2c.weight', 'encoder_encoder.ff_3.w_2c.bias', 'f_tag.0.weight', 'encoder_encoder.ff_0.w_2c.weight', 'encoder_encoder.attn_6.proj1.weight', 'encoder_encoder.ff_1.w_2p.bias', 'embedding.layer_norm.a_2', 'encoder_encoder.attn_1.w_qs2', 'encoder_encoder.ff_2.layer_norm.a_2', 'encoder_encoder.attn_1.layer_norm.a_2', 'encoder_encoder.ff_3.w_1p.weight', 'encoder_encoder.attn_0.proj1.weight', 'encoder_encoder.ff_2.w_2c.weight', 'encoder_encoder.attn_2.w_qs1', 'encoder_encoder.attn_5.w_qs1', 'encoder_encoder.ff_3.w_1c.weight', 'encoder_encoder.attn_7.w_vs2', 'encoder_encoder.attn_3.layer_norm.a_2', 'encoder_encoder.attn_0.w_vs2', 'encoder_encoder.ff_2.w_1p.bias', 'encoder_encoder.ff_0.layer_norm.a_2', 'encoder_encoder.attn_1.w_vs1', 'encoder_encoder.ff_2.w_1c.bias', 'encoder_encoder.attn_4.layer_norm.b_2', 'encoder_encoder.ff_2.w_2c.bias', 'encoder_encoder.ff_4.layer_norm.b_2', 'encoder_encoder.ff_6.w_1p.bias', 'encoder_encoder.ff_2.layer_norm.b_2', 'encoder_encoder.ff_3.w_2c.weight', 'encoder_encoder.ff_6.w_2c.bias', 'encoder_encoder.ff_1.layer_norm.b_2', 'encoder_encoder.ff_0.w_1p.weight', 'encoder_encoder.attn_2.layer_norm.b_2', 'embedding.layer_norm.b_2', 'encoder_encoder.ff_7.w_2c.bias', 'encoder_encoder.attn_0.layer_norm.a_2', 'encoder_encoder.attn_5.w_qs2', 'encoder_encoder.ff_6.w_1c.bias', 'encoder_encoder.attn_3.w_ks2', 'encoder_encoder.attn_3.proj2.weight', 'encoder_encoder.ff_4.w_1p.bias', 'encoder_encoder.attn_4.w_qs2', 'encoder_encoder.ff_7.layer_norm.a_2', 'encoder_encoder.attn_2.w_ks2', 'encoder_encoder.ff_3.layer_norm.a_2', 'encoder_encoder.ff_2.w_2p.weight', 'encoder_encoder.ff_6.w_2p.bias', 'encoder_encoder.ff_7.layer_norm.b_2', 'encoder_encoder.ff_4.w_2p.weight', 'encoder_encoder.attn_3.w_qs1', 'encoder_encoder.ff_3.w_2p.bias', 'encoder_encoder.attn_0.proj2.weight', 'encoder_encoder.ff_5.layer_norm.a_2', 'encoder_encoder.ff_1.w_2p.weight', 'encoder_encoder.attn_4.w_vs1', 'encoder_encoder.attn_6.layer_norm.a_2', 'encoder_encoder.ff_4.w_1p.weight', 'encoder_encoder.attn_3.w_qs2', 'encoder_encoder.attn_6.w_qs1', 'encoder_encoder.ff_5.w_1p.weight', 'encoder_encoder.attn_1.proj2.weight', 'encoder_encoder.attn_0.w_ks2', 'encoder_encoder.ff_1.w_1c.weight', 'encoder_encoder.ff_5.w_2c.bias', 'encoder_encoder.ff_7.w_1c.weight', 'encoder_encoder.ff_4.w_2c.weight', 'encoder_encoder.attn_5.w_ks2', 'encoder_encoder.ff_2.w_1c.weight', 'encoder_encoder.attn_7.w_qs1', 'encoder_encoder.attn_2.w_vs2', 'encoder_encoder.attn_5.w_ks1', 'f_tag.3.bias', 'encoder_encoder.attn_5.proj2.weight', 'f_tag.1.a_2', 'encoder_encoder.attn_4.proj2.weight', 'encoder_encoder.ff_0.layer_norm.b_2', 'encoder_encoder.ff_3.w_2p.weight', 'encoder_encoder.ff_1.layer_norm.a_2', 'encoder_encoder.attn_4.proj1.weight', 'encoder_encoder.ff_5.w_2p.weight', 'encoder_encoder.attn_6.w_ks2', 'encoder_encoder.ff_6.w_1c.weight', 'encoder_encoder.attn_6.w_qs2', 'encoder_encoder.attn_4.w_ks2', 'encoder_encoder.attn_7.w_ks2', 'encoder_encoder.ff_2.w_2p.bias', 'encoder_encoder.attn_7.layer_norm.a_2', 'encoder_encoder.attn_6.w_vs1', 'encoder_encoder.ff_3.w_1c.bias', 'f_label.1.a_2', 'encoder_encoder.attn_6.layer_norm.b_2', 'f_tag.3.weight', 'encoder_encoder.ff_3.layer_norm.b_2', 'encoder_encoder.ff_4.w_1c.weight', 'f_label.0.bias', 'encoder_encoder.attn_0.layer_norm.b_2', 'encoder_encoder.attn_0.w_qs2', 'encoder_encoder.attn_7.w_vs1', 'encoder_encoder.attn_4.w_ks1', 'f_label.1.b_2', 'encoder_encoder.ff_4.layer_norm.a_2', 'encoder_encoder.attn_2.w_ks1', 'encoder_encoder.attn_3.w_ks1', 'f_tag.1.b_2', 'encoder_encoder.attn_1.proj1.weight', 'encoder_encoder.ff_0.w_1c.weight', 'encoder_encoder.ff_1.w_2c.weight', 'encoder_encoder.ff_0.w_1c.bias', 'encoder_encoder.ff_5.w_2p.bias', 'encoder_encoder.attn_1.w_ks1', 'encoder_encoder.ff_2.w_1p.weight', 'encoder_encoder.attn_3.w_vs1', 'encoder_encoder.ff_5.w_1p.bias', 'encoder_encoder.ff_7.w_2c.weight', 'encoder_encoder.attn_2.w_qs2', 'encoder_encoder.attn_4.w_qs1', 'encoder_encoder.attn_5.layer_norm.a_2', 'encoder_encoder.ff_0.w_1p.bias', 'f_label.3.bias', 'encoder_encoder.ff_3.w_1p.bias', 'encoder_encoder.attn_0.w_qs1', 'encoder_encoder.ff_6.w_1p.weight', 'encoder_encoder.attn_4.w_vs2', 'encoder_encoder.attn_2.proj1.weight', 'f_label.0.weight', 'encoder_encoder.attn_7.w_ks1', 'encoder_encoder.attn_3.proj1.weight', 'encoder_encoder.attn_6.w_ks1', 'project_bert.weight', 'encoder_encoder.attn_7.layer_norm.b_2', 'encoder_encoder.ff_5.w_2c.weight', 'encoder_encoder.attn_1.w_vs2', 'encoder_encoder.attn_1.w_ks2', 'encoder_encoder.ff_4.w_2p.bias', 'encoder_encoder.ff_7.w_1p.bias', 'encoder_encoder.ff_7.w_1p.weight', 'encoder_encoder.attn_6.proj2.weight', 'encoder_encoder.attn_5.w_vs2', 'encoder_encoder.ff_4.w_1c.bias', 'encoder_encoder.ff_5.w_1c.bias', 'encoder_encoder.attn_7.proj1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = T5Constituency.from_pretrained('mesolitica/nanot5-base-malaysian-cased', config = config)\n",
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_parameters = [param for param in model.parameters() if param.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.AdamW(trainable_parameters, lr = 2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchIndices:\n",
    "    def __init__(self, batch_idxs_np):\n",
    "        self.batch_idxs_np = batch_idxs_np\n",
    "        self.batch_idxs_torch = torch.from_numpy(batch_idxs_np)\n",
    "        self.batch_size = int(1 + np.max(batch_idxs_np))\n",
    "\n",
    "        batch_idxs_np_extra = np.concatenate([[-1], batch_idxs_np, [-1]])\n",
    "        self.boundaries_np = np.nonzero(batch_idxs_np_extra[1:] != batch_idxs_np_extra[:-1])[0]\n",
    "        self.seq_lens_np = self.boundaries_np[1:] - self.boundaries_np[:-1]\n",
    "        assert len(self.seq_lens_np) == self.batch_size\n",
    "        self.max_len = int(np.max(self.boundaries_np[1:] - self.boundaries_np[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(sentences, golds, subbatch_max_tokens=3000):\n",
    "    lens = [\n",
    "        len(tokenizer.tokenize(' '.join([word for (_, word) in sentence]))) + 2\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "\n",
    "    lens = np.asarray(lens, dtype=int)\n",
    "    lens_argsort = np.argsort(lens).tolist()\n",
    "\n",
    "    num_subbatches = 0\n",
    "    subbatch_size = 1\n",
    "    while lens_argsort:\n",
    "        if (subbatch_size == len(lens_argsort)) or (subbatch_size * lens[lens_argsort[subbatch_size]] > subbatch_max_tokens):\n",
    "            yield [sentences[i] for i in lens_argsort[:subbatch_size]], [golds[i] for i in lens_argsort[:subbatch_size]]\n",
    "            lens_argsort = lens_argsort[subbatch_size:]\n",
    "            num_subbatches += 1\n",
    "            subbatch_size = 1\n",
    "        else:\n",
    "            subbatch_size += 1\n",
    "            \n",
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(\n",
    "            sentence + [pad_int] * (max_sentence_len - len(sentence))\n",
    "        )\n",
    "        seq_lens.append(len(sentence))\n",
    "    return padded_seqs, seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(sentences, golds = None):\n",
    "    \n",
    "    all_input_ids = []\n",
    "    all_word_start_mask = []\n",
    "    all_word_end_mask = []\n",
    "\n",
    "    for snum, sentence in enumerate(sentences):\n",
    "\n",
    "        tokens = []\n",
    "        word_start_mask = []\n",
    "        word_end_mask = []\n",
    "        tokens.append(START)\n",
    "        word_start_mask.append(1)\n",
    "        word_end_mask.append(1)\n",
    "\n",
    "        cleaned_words = []\n",
    "        for _, word in sentence:\n",
    "            cleaned_words.append(process_word(word))\n",
    "\n",
    "        for word in cleaned_words:\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            for _ in range(len(word_tokens)):\n",
    "                word_start_mask.append(0)\n",
    "                word_end_mask.append(0)\n",
    "            word_start_mask[len(tokens)] = 1\n",
    "            word_end_mask[-1] = 1\n",
    "            tokens.extend(word_tokens)\n",
    "        tokens.append(STOP)\n",
    "        word_start_mask.append(1)\n",
    "        word_end_mask.append(1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_word_start_mask.append(word_start_mask)\n",
    "        all_word_end_mask.append(word_end_mask)\n",
    "    \n",
    "    padded = tokenizer.pad({\n",
    "        'input_ids': all_input_ids,\n",
    "    }, return_tensors = 'pt')\n",
    "\n",
    "    all_word_start_mask = torch.from_numpy(np.array(pad_sentence_batch(all_word_start_mask, 0)[0]))\n",
    "    all_word_end_mask = torch.from_numpy(np.array(pad_sentence_batch(all_word_end_mask, 0)[0]))\n",
    "    \n",
    "    padded['sentences'] = sentences\n",
    "    padded['all_word_start_mask'] = all_word_start_mask\n",
    "    padded['all_word_end_mask'] = all_word_end_mask\n",
    "    \n",
    "    packed_len = sum([(len(sentence) + 2) for sentence in sentences])\n",
    "    i = 0\n",
    "    tag_idxs = np.zeros(packed_len, dtype=int)\n",
    "    batch_idxs = np.zeros(packed_len, dtype=int)\n",
    "    for snum, sentence in enumerate(sentences):\n",
    "        for (tag, word) in [(START, START)] + sentence + [(STOP, STOP)]:\n",
    "            if golds is not None:\n",
    "                tag_idxs[i] = tag_vocab.index_or_unk(tag, TAG_UNK)\n",
    "            else:\n",
    "                tag_idxs[i] = 0\n",
    "            batch_idxs[i] = snum\n",
    "            i += 1\n",
    "    \n",
    "    batch_idxs = BatchIndices(batch_idxs)\n",
    "    padded['batch_idxs'] = batch_idxs\n",
    "    tag_idxs = torch.from_numpy(tag_idxs)\n",
    "    padded['tag_idxs'] = tag_idxs\n",
    "    \n",
    "    if golds is not None:\n",
    "        gold_tag_idxs = tag_idxs\n",
    "        padded['gold_tag_idxs'] = gold_tag_idxs\n",
    "        padded['golds'] = golds\n",
    "        \n",
    "        \n",
    "    if torch.cuda.is_available():\n",
    "        for k in padded.keys():\n",
    "            if isinstance(padded[k], torch.Tensor):\n",
    "                padded[k] = padded[k].cuda()\n",
    "    \n",
    "        padded['batch_idxs'].batch_idxs_torch = padded['batch_idxs'].batch_idxs_torch.cuda()\n",
    "    \n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(268.9514, device='cuda:0', grad_fn=<AddBackward0>), tensor(278.9067, device='cuda:0', grad_fn=<MulBackward0>))\n"
     ]
    }
   ],
   "source": [
    "start_index = 0\n",
    "batch_size = 4\n",
    "batch_trees = train_parse[start_index:start_index + batch_size]\n",
    "batch_sentences = [[(leaf.tag, leaf.word) for leaf in tree.leaves()] for tree in batch_trees]\n",
    "for subbatch_sentences, subbatch_trees in split_batch(batch_sentences, batch_trees):\n",
    "    print(model(**process(subbatch_sentences, subbatch_trees)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model(**process(subbatch_sentences)), model(**process(subbatch_sentences, subbatch_trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:07<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 0, loss: 8.819639983177185, dev_fscore: (Recall=66.51, Precision=70.94, FScore=68.65, CompleteMatch=6.90, TaggingAccuracy=93.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 1, loss: 4.456185948133468, dev_fscore: (Recall=70.55, Precision=71.62, FScore=71.08, CompleteMatch=10.20, TaggingAccuracy=93.63)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 2, loss: 3.1180720081329345, dev_fscore: (Recall=73.64, Precision=75.03, FScore=74.33, CompleteMatch=12.40, TaggingAccuracy=93.80)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 3, loss: 2.430565314412117, dev_fscore: (Recall=74.54, Precision=73.88, FScore=74.21, CompleteMatch=14.00, TaggingAccuracy=93.50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 4, loss: 1.9220690823793412, dev_fscore: (Recall=76.17, Precision=76.18, FScore=76.17, CompleteMatch=16.30, TaggingAccuracy=93.71)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 5, loss: 1.5730757367014885, dev_fscore: (Recall=77.61, Precision=77.25, FScore=77.43, CompleteMatch=14.70, TaggingAccuracy=93.87)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:06<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 6, loss: 1.3969583784341812, dev_fscore: (Recall=76.75, Precision=77.46, FScore=77.10, CompleteMatch=16.30, TaggingAccuracy=93.85)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:04<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 7, loss: 1.207198276937008, dev_fscore: (Recall=77.30, Precision=78.27, FScore=77.78, CompleteMatch=16.20, TaggingAccuracy=93.81)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 8, loss: 1.1719542775154115, dev_fscore: (Recall=78.59, Precision=78.16, FScore=78.37, CompleteMatch=17.60, TaggingAccuracy=93.93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 9, loss: 1.0005309571921825, dev_fscore: (Recall=78.70, Precision=76.81, FScore=77.75, CompleteMatch=16.50, TaggingAccuracy=93.78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 10, loss: 0.9286423883438111, dev_fscore: (Recall=79.51, Precision=78.38, FScore=78.94, CompleteMatch=18.60, TaggingAccuracy=93.97)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 11, loss: 0.8747475933432579, dev_fscore: (Recall=79.34, Precision=78.36, FScore=78.84, CompleteMatch=18.60, TaggingAccuracy=94.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 12, loss: 0.8335441668629646, dev_fscore: (Recall=79.42, Precision=78.98, FScore=79.20, CompleteMatch=18.80, TaggingAccuracy=94.09)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 13, loss: 0.8101441066265106, dev_fscore: (Recall=78.95, Precision=78.89, FScore=78.92, CompleteMatch=18.30, TaggingAccuracy=93.90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 14, loss: 0.7101925003826618, dev_fscore: (Recall=79.36, Precision=79.43, FScore=79.39, CompleteMatch=19.10, TaggingAccuracy=94.27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 15, loss: 0.675981358319521, dev_fscore: (Recall=80.10, Precision=79.61, FScore=79.85, CompleteMatch=20.70, TaggingAccuracy=94.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 16, loss: 0.6253926932662726, dev_fscore: (Recall=79.74, Precision=78.88, FScore=79.31, CompleteMatch=18.70, TaggingAccuracy=94.02)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 17, loss: 0.6020984571874142, dev_fscore: (Recall=81.04, Precision=79.03, FScore=80.02, CompleteMatch=20.80, TaggingAccuracy=94.28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 18, loss: 0.5756492719501257, dev_fscore: (Recall=79.87, Precision=79.83, FScore=79.85, CompleteMatch=20.50, TaggingAccuracy=94.24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 19, loss: 0.5746373542696237, dev_fscore: (Recall=79.38, Precision=79.91, FScore=79.65, CompleteMatch=22.40, TaggingAccuracy=94.27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 20, loss: 0.5201877975389362, dev_fscore: (Recall=79.36, Precision=79.84, FScore=79.60, CompleteMatch=19.50, TaggingAccuracy=93.86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 21, loss: 0.4912329556792974, dev_fscore: (Recall=80.15, Precision=80.23, FScore=80.19, CompleteMatch=21.40, TaggingAccuracy=94.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 22, loss: 0.4710603418499231, dev_fscore: (Recall=80.03, Precision=79.97, FScore=80.00, CompleteMatch=21.40, TaggingAccuracy=94.09)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 23, loss: 0.4637663812190294, dev_fscore: (Recall=80.32, Precision=79.65, FScore=79.99, CompleteMatch=19.10, TaggingAccuracy=94.23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 24, loss: 0.46933632941544057, dev_fscore: (Recall=80.85, Precision=79.59, FScore=80.21, CompleteMatch=20.60, TaggingAccuracy=94.34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 25, loss: 0.5320658619701862, dev_fscore: (Recall=80.34, Precision=79.97, FScore=80.15, CompleteMatch=20.70, TaggingAccuracy=94.38)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 26, loss: 0.4144169696420431, dev_fscore: (Recall=79.75, Precision=81.32, FScore=80.53, CompleteMatch=21.40, TaggingAccuracy=94.35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 27, loss: 0.38793145314604044, dev_fscore: (Recall=80.74, Precision=81.15, FScore=80.94, CompleteMatch=22.50, TaggingAccuracy=94.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 28, loss: 0.4557119156047702, dev_fscore: (Recall=80.03, Precision=80.47, FScore=80.25, CompleteMatch=22.60, TaggingAccuracy=94.27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 29, loss: 0.3504318243339658, dev_fscore: (Recall=80.55, Precision=80.12, FScore=80.33, CompleteMatch=22.50, TaggingAccuracy=94.02)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:06<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 30, loss: 0.39788834031671283, dev_fscore: (Recall=80.85, Precision=80.93, FScore=80.89, CompleteMatch=22.10, TaggingAccuracy=94.35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 31, loss: 0.37517670648172496, dev_fscore: (Recall=81.00, Precision=79.60, FScore=80.29, CompleteMatch=21.40, TaggingAccuracy=94.36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 32, loss: 0.3500702136158943, dev_fscore: (Recall=80.75, Precision=80.45, FScore=80.60, CompleteMatch=22.50, TaggingAccuracy=94.40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 33, loss: 0.3293591633290052, dev_fscore: (Recall=80.96, Precision=80.93, FScore=80.95, CompleteMatch=21.70, TaggingAccuracy=94.41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 34, loss: 0.3103742697276175, dev_fscore: (Recall=80.54, Precision=80.38, FScore=80.46, CompleteMatch=22.30, TaggingAccuracy=94.27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 35, loss: 0.3065598714016378, dev_fscore: (Recall=81.03, Precision=80.54, FScore=80.79, CompleteMatch=22.80, TaggingAccuracy=94.51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 36, loss: 0.3278788373600692, dev_fscore: (Recall=80.10, Precision=80.81, FScore=80.45, CompleteMatch=21.20, TaggingAccuracy=94.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 37, loss: 0.3105921440348029, dev_fscore: (Recall=80.75, Precision=80.88, FScore=80.81, CompleteMatch=23.00, TaggingAccuracy=94.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 38, loss: 0.2826378598809242, dev_fscore: (Recall=79.74, Precision=80.76, FScore=80.25, CompleteMatch=21.90, TaggingAccuracy=94.06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 39, loss: 0.3209829128049314, dev_fscore: (Recall=80.94, Precision=80.96, FScore=80.95, CompleteMatch=23.10, TaggingAccuracy=94.43)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 40, loss: 0.2583822923637927, dev_fscore: (Recall=80.89, Precision=81.87, FScore=81.38, CompleteMatch=23.90, TaggingAccuracy=94.45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 41, loss: 0.2601054233200848, dev_fscore: (Recall=80.11, Precision=81.34, FScore=80.72, CompleteMatch=22.40, TaggingAccuracy=94.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 42, loss: 0.24275643637776376, dev_fscore: (Recall=79.67, Precision=81.57, FScore=80.61, CompleteMatch=21.90, TaggingAccuracy=94.35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 43, loss: 0.24849758695811033, dev_fscore: (Recall=80.16, Precision=81.45, FScore=80.80, CompleteMatch=21.60, TaggingAccuracy=94.36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 44, loss: 0.23292735051363705, dev_fscore: (Recall=80.08, Precision=80.11, FScore=80.10, CompleteMatch=20.20, TaggingAccuracy=94.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 45, loss: 0.2826069640889764, dev_fscore: (Recall=80.98, Precision=81.30, FScore=81.14, CompleteMatch=24.00, TaggingAccuracy=94.29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 46, loss: 0.2309159503504634, dev_fscore: (Recall=79.95, Precision=81.88, FScore=80.90, CompleteMatch=22.40, TaggingAccuracy=94.32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 47, loss: 0.19940490192826837, dev_fscore: (Recall=79.70, Precision=82.10, FScore=80.89, CompleteMatch=24.00, TaggingAccuracy=94.53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 48, loss: 0.2133789891451597, dev_fscore: (Recall=78.65, Precision=82.14, FScore=80.35, CompleteMatch=22.50, TaggingAccuracy=94.29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 49, loss: 0.23868915298767387, dev_fscore: (Recall=80.15, Precision=80.80, FScore=80.47, CompleteMatch=20.80, TaggingAccuracy=94.41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:05<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch: 50, loss: 0.2080194965712726, dev_fscore: (Recall=79.93, Precision=81.58, FScore=80.75, CompleteMatch=22.40, TaggingAccuracy=94.43)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "epoch = 100\n",
    "\n",
    "best_dev_fscore = -np.inf\n",
    "patient = 10\n",
    "current_patient = 0\n",
    "\n",
    "for e in range(epoch):\n",
    "    pbar = tqdm(range(0, len(train_parse), batch_size))\n",
    "    losses = []\n",
    "    for start_index in pbar:\n",
    "        trainer.zero_grad()\n",
    "        batch_loss_value = 0.0\n",
    "        batch_trees = train_parse[start_index:start_index + batch_size]\n",
    "        batch_sentences = [[(leaf.tag, leaf.word) for leaf in tree.leaves()] for tree in batch_trees]\n",
    "        batch_num_tokens = sum(len(sentence) for sentence in batch_sentences)\n",
    "\n",
    "        for subbatch_sentences, subbatch_trees in split_batch(batch_sentences, batch_trees):\n",
    "            loss, tag_loss =  model(**process(subbatch_sentences, subbatch_trees))\n",
    "            loss = tag_loss / len(subbatch_sentences) + loss / batch_num_tokens\n",
    "            loss_value = float(loss.data.cpu().numpy())\n",
    "            batch_loss_value += loss_value\n",
    "            if loss_value > 0:\n",
    "                loss.backward()\n",
    "        \n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(trainable_parameters, 5.0)\n",
    "        trainer.step()\n",
    "        losses.append(batch_loss_value)\n",
    "        \n",
    "    dev_predicted = []\n",
    "    for dev_start_index in range(0, len(dev_treebank), batch_size):\n",
    "        subbatch_trees = dev_treebank[dev_start_index:dev_start_index+batch_size]\n",
    "        subbatch_sentences = [[(leaf.tag, leaf.word) for leaf in tree.leaves()] for tree in subbatch_trees]\n",
    "        predicted, _ =  model(**process(subbatch_sentences))\n",
    "        dev_predicted.extend([p.convert() for p in predicted])\n",
    "    \n",
    "    dev_fscore = evaluate.evalb('deprecated/EVALB', dev_treebank, dev_predicted)\n",
    "    \n",
    "    print(f'epoch: {e}, loss: {np.mean(losses)}, dev_fscore: {dev_fscore}')\n",
    "    \n",
    "    if dev_fscore.fscore >= best_dev_fscore:\n",
    "        best_dev_fscore = dev_fscore.fscore\n",
    "        current_patient = 0\n",
    "        model.save_pretrained('base')\n",
    "    else:\n",
    "        current_patient += 1\n",
    "    \n",
    "    if current_patient >= patient:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = T5Constituency.from_pretrained('./base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b334af51c84127bf20fe42b4931636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/545M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/constituency-parsing-nanot5-base-malaysian-cased/commit/c3d2dff23d539dad56f6cd12202d7c711782c236', commit_message='Upload T5Constituency', commit_description='', oid='c3d2dff23d539dad56f6cd12202d7c711782c236', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_.push_to_hub('mesolitica/constituency-parsing-nanot5-base-malaysian-cased', safe_serialization = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/constituency-parsing-nanot5-base-malaysian-cased/commit/bbcfb2dd265b33505c80f5fc12f005b7c3a70ace', commit_message='Upload tokenizer', commit_description='', oid='bbcfb2dd265b33505c80f5fc12f005b7c3a70ace', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('mesolitica/constituency-parsing-nanot5-base-malaysian-cased', safe_serialization = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
