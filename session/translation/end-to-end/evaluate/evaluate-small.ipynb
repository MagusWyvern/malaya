{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/mesolitica/malaysian-dataset/raw/master/translation/noisy-eval/kopitiam.json_0_to_1000-test.json\n",
    "# !wget https://github.com/mesolitica/malaysian-dataset/raw/master/translation/noisy-eval/twitter.json_0_to_1000-test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/mesolitica/malaysian-dataset/raw/master/translation/flores200-eval/bjn_Latn.dev\n",
    "# !wget https://github.com/mesolitica/malaysian-dataset/raw/master/translation/flores200-eval/eng_Latn.dev\n",
    "# !wget https://github.com/mesolitica/malaysian-dataset/raw/master/translation/flores200-eval/ind_Latn.dev\n",
    "# !wget https://github.com/mesolitica/malaysian-dataset/raw/master/translation/flores200-eval/jav_Latn.dev\n",
    "# !wget https://github.com/mesolitica/malaysian-dataset/raw/master/translation/flores200-eval/zsm_Latn.dev\n",
    "# !wget https://github.com/mesolitica/malaysian-dataset/raw/master/translation/flores200-eval/zho_Hans.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 16:27:16.547815: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-06 16:27:16.624418: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-06 16:27:17.028985: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-06 16:27:17.029031: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-06 16:27:17.029035: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('mesolitica/nanot5-base-malaysian-cased')\n",
    "model = T5ForConditionalGeneration.from_pretrained('mesolitica/translation-nanot5-small-malaysian-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hai, ada apa yang boleh saya bantu?']\n"
     ]
    }
   ],
   "source": [
    "s = 'Hai, ada yang bisa saya bantu?'\n",
    "input_ids = tokenizer.encode(f'terjemah ke Melayu: {s}{tokenizer.eos_token}', return_tensors = 'pt').cuda()\n",
    "outputs = model.generate(input_ids, max_length = 100)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\n",
    "    'bjn': 'bjn_Latn.dev',\n",
    "    'en': 'eng_Latn.dev',\n",
    "    'ind': 'ind_Latn.dev',\n",
    "    'jav': 'jav_Latn.dev',\n",
    "    'ms': 'zsm_Latn.dev',\n",
    "    'mandarin': 'zho_Hans.dev',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bjn 997\n",
      "en 997\n",
      "ind 997\n",
      "jav 997\n",
      "ms 997\n",
      "mandarin 997\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for k, v in files.items():\n",
    "    with open(v) as fopen:\n",
    "        t = fopen.read().split('\\n')\n",
    "    \n",
    "    t = [s for s in t if len(s)]\n",
    "    data[k] = t\n",
    "    print(k, len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ['en', 'ms'],\n",
    "    ['ms', 'en'],\n",
    "    ['ind', 'ms'],\n",
    "    ['jav', 'ms'],\n",
    "    ['mandarin', 'ms'],\n",
    "    ['mandarin', 'en']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "\n",
    "chrf = CHRF(word_order = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'ms': 'Melayu',\n",
    "    'en': 'Inggeris',\n",
    "    'manglish': 'Manglish',\n",
    "    'pasar ms': 'pasar Melayu',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 997/997 [01:19<00:00, 12.49it/s]\n",
      "100%|█████████████████████████████████████████| 997/997 [01:20<00:00, 12.40it/s]\n",
      "100%|█████████████████████████████████████████| 997/997 [01:20<00:00, 12.34it/s]\n",
      "100%|█████████████████████████████████████████| 997/997 [01:22<00:00, 12.07it/s]\n",
      "100%|█████████████████████████████████████████| 997/997 [01:26<00:00, 11.53it/s]\n",
      "100%|█████████████████████████████████████████| 997/997 [01:40<00:00,  9.90it/s]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for left, right in pairs:\n",
    "    predicted = []\n",
    "    l = mapping[right]\n",
    "    for s in tqdm(data[left]):\n",
    "        input_ids = tokenizer.encode(f'terjemah ke {l}: {s}{tokenizer.eos_token}', return_tensors = 'pt').cuda()\n",
    "        outputs = model.generate(input_ids, max_length = 1024)\n",
    "        predicted.append(tokenizer.batch_decode(outputs, skip_special_tokens = True)[0])\n",
    "    \n",
    "    score = chrf.corpus_score(predicted, [data[right][:len(predicted)]])\n",
    "    results[f'{left}-{right}'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en-ms': chrF2++ = 66.98,\n",
       " 'ms-en': chrF2++ = 63.52,\n",
       " 'ind-ms': chrF2++ = 58.10,\n",
       " 'jav-ms': chrF2++ = 51.55,\n",
       " 'mandarin-ms': chrF2++ = 46.09,\n",
       " 'mandarin-en': chrF2++ = 44.13}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "986"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('kopitiam.json_0_to_1000-test.json') as fopen:\n",
    "    data = json.load(fopen)\n",
    "    \n",
    "manglish = []\n",
    "for d in data:\n",
    "    try:\n",
    "        r = json.loads(d[1])\n",
    "        d = {\n",
    "            'manglish': d[0],\n",
    "            'ms': r['malay'],\n",
    "            'en': r['british_english'],\n",
    "        }\n",
    "        if isinstance(d['manglish'], str) and isinstance(d['ms'], str) and isinstance(d['en'], str):\n",
    "            manglish.append(d)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "data = pd.DataFrame(manglish).to_dict(orient = 'list')\n",
    "len(data['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ['manglish', 'ms'],\n",
    "    ['manglish', 'en'],\n",
    "    ['ms', 'manglish'],\n",
    "    ['en', 'manglish'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 986/986 [01:52<00:00,  8.77it/s]\n",
      "100%|█████████████████████████████████████████| 986/986 [01:37<00:00, 10.11it/s]\n",
      "100%|█████████████████████████████████████████| 986/986 [01:46<00:00,  9.25it/s]\n",
      "100%|█████████████████████████████████████████| 986/986 [01:26<00:00, 11.37it/s]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for left, right in pairs:\n",
    "    predicted = []\n",
    "    l = mapping[right]\n",
    "    for s in tqdm(data[left]):\n",
    "        input_ids = tokenizer.encode(f'terjemah ke {l}: {s}{tokenizer.eos_token}', return_tensors = 'pt').cuda()\n",
    "        outputs = model.generate(input_ids, max_length = 1024)\n",
    "        predicted.append(tokenizer.batch_decode(outputs, skip_special_tokens = True)[0])\n",
    "    \n",
    "    score = chrf.corpus_score(predicted, [data[right][:len(predicted)]])\n",
    "    results[f'{left}-{right}'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'manglish-ms': chrF2++ = 54.09,\n",
       " 'manglish-en': chrF2++ = 55.27,\n",
       " 'ms-manglish': chrF2++ = 40.19,\n",
       " 'en-manglish': chrF2++ = 45.69}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "991"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('twitter.json_0_to_1000-test.json') as fopen:\n",
    "    data = json.load(fopen)\n",
    "\n",
    "twitter = []\n",
    "for d in data:\n",
    "    try:\n",
    "        r = json.loads(d[1])\n",
    "        d = {\n",
    "            'pasar ms': d[0],\n",
    "            'ms': r['malay'],\n",
    "            'en': r['english'],\n",
    "        }\n",
    "        if isinstance(d['pasar ms'], str) and isinstance(d['ms'], str) and isinstance(d['en'], str):\n",
    "            twitter.append(d)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "data = pd.DataFrame(twitter).to_dict(orient = 'list')\n",
    "len(data['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ['pasar ms', 'ms'],\n",
    "    ['pasar ms', 'en'],\n",
    "    ['ms', 'pasar ms'],\n",
    "    ['en', 'pasar ms'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 991/991 [01:18<00:00, 12.68it/s]\n",
      "100%|█████████████████████████████████████████| 991/991 [01:38<00:00, 10.09it/s]\n",
      "100%|█████████████████████████████████████████| 991/991 [02:07<00:00,  7.77it/s]\n",
      "100%|█████████████████████████████████████████| 991/991 [01:58<00:00,  8.33it/s]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for left, right in pairs:\n",
    "    predicted = []\n",
    "    l = mapping[right]\n",
    "    for s in tqdm(data[left]):\n",
    "        input_ids = tokenizer.encode(f'terjemah ke {l}: {s}{tokenizer.eos_token}', return_tensors = 'pt').cuda()\n",
    "        outputs = model.generate(input_ids, max_length = 1024)\n",
    "        predicted.append(tokenizer.batch_decode(outputs, skip_special_tokens = True)[0])\n",
    "    \n",
    "    score = chrf.corpus_score(predicted, [data[right][:len(predicted)]])\n",
    "    results[f'{left}-{right}'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pasar ms-ms': chrF2++ = 63.20,\n",
       " 'pasar ms-en': chrF2++ = 59.78,\n",
       " 'ms-pasar ms': chrF2++ = 55.09,\n",
       " 'en-pasar ms': chrF2++ = 53.01}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
