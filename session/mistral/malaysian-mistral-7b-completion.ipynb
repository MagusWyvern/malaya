{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222a5c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('mesolitica/mistral-7b-4096-fpf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f682ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type='nf4',\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21e8eb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4a6a5532e447309452daeadf8d0af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/611 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-21 13:25:10,069] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 13:25:10.415860: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-21 13:25:11.085951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8327d3b889624862a6b5ce5f4c34912f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5722d78fdf4c7b816841179aad6e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5d96c1863645a1b5699fee2f81b1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cea18a9e889462089827d8b41427482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'mesolitica/mistral-7b-4096-fpf', \n",
    "    quantization_config=nf4_config,\n",
    "    use_flash_attention_2=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81fb313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "kwargs = {\n",
    "    'temperature': 0.9, \n",
    "    'max_new_tokens': 512, \n",
    "    'top_p': 0.95,\n",
    "    'repetition_penalty': 1.1, \n",
    "    'do_sample': True,\n",
    "    'num_beams': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bd6b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(['Manusia: macam mana nak code websocket guna htmx? Bot:'], return_tensors='pt').to('cuda')\n",
    "generate_kwargs = dict(inputs)\n",
    "generate_kwargs = {**generate_kwargs, **kwargs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5457089a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> Manusia: macam mana nak code websocket guna htmx? Bot: Hanya dengan menggunakan JavaScript dan HTML, anda boleh menghantar data dari bot ke pelayan menggunakan WebSocket. Berikut adalah contoh kod untuk melaksanakannya: ``` var ws = new WebSocket.WebSocket(\"ws://your-website/\"); ws.onopen = function() { // Menyambung ke pelayan }; ws.onmessage = function(event) { // Menerima data dari pelayan }; ws.onclose = function() { // Menutup sambungan }; var message = \"Hello World!\"; // Mesej untuk dihantar ke pelayan ws.send(message); ``` Dalam kod ini, kami membuat objek `WebSocket` baru menggunakan URL pelayan WebSocket. Kemudian, kami menentukan pendengar acara untuk acara `open`, `message`, dan `close`. Fungsi pendengar `open` dipicu apabila sambungan ke pelayan berjaya. Dalam fungsi ini, anda boleh menetapkan WebSocket supaya dapat menerima mesej yang masuk. Fungsi pendengar `message` dipicu apabila mesej diterima dari pelayan. Anda boleh menerima mesej yang masuk dan menguraikan data dalam fungsi ini. Fungsi pendengar `close` dipicu apabila sambungan ke pelayan terputus. Ini bermakna pengguna telah meninggalkan halaman, dan bot boleh berehat atau tidur selama itu. Untuk menghantar mesej ke pelayan, anda boleh menggunakan fungsi `send` yang disediakan oleh objek `WebSocket`. Hanya perlu memasukkan mesej sebagai rentetan (dalam kes ini, \"Hello World!\"). Setelah pelayan menerima mesej, ia akan memproses dan'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = model.generate(**generate_kwargs)\n",
    "tokenizer.decode(o[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93f98d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> Manusia: macam mana nak code sentiment guna malaya? Bot: Dengan menggunakan API Pengesanan Emosi, kita boleh melatih model pembelajaran mesin untuk mengenali emosi dalam teks. Dalam bahasa pengaturcaraan Python, ini boleh dicapai dengan menggunakan perpustakaan Natural Language Processing (NLP) seperti NLTK dan Stanford NER. Berikut adalah contoh bagaimana untuk memproses data sentimen menggunakan algoritma NLTK: ```python import nltk from nltk.sentiment import SentimentIntensityAnalyzer # muat turun leksikon yang diperlukan untuk analisis sentimen nltk.download(\\'vader_lexicon\\') sid = SentimentIntensityAnalyzer() # annotate rentetan \"Saya suka produk ini\" data_sentimen = sid.polarity_scores(\"Saya suka produk ini\") # output: {\\'neg\\': 0.0, \\'neu\\': 0.5, \\'pos\\': 0.5, \\'compound\\': 1.0} ``` Untuk menambahkan keupayaan manusia dalam proses ini, kita boleh melatih model pembelajaran mesin menggunakan senarai sentimen yang diberikan oleh manusia. Setelah model dilatih, ia boleh meramalkan sentimen teks yang diberikan oleh pengguna. Sebagai contoh, jika pengguna ingin memberikan ulasan mengenai produk, kita boleh mencadangkan sentimen alternatif dan memberi mereka skor keyakinan untuk setiap pilihan. Cadangan ini akan dipacu oleh sentimen analisis serta pandangan peribadi manusia mengenai teks input. Dengan cara ini, sistem cadangan boleh menjadi lebih komprehensif dan tepat.</s>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(['Manusia: macam mana nak code sentiment guna malaya? Bot:'], return_tensors='pt').to('cuda')\n",
    "generate_kwargs = dict(inputs)\n",
    "generate_kwargs = {**generate_kwargs, **kwargs}\n",
    "o = model.generate(**generate_kwargs)\n",
    "tokenizer.decode(o[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87fe9e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Manusia: macam mana nak code sentiment guna malaya? Bot: Dengan menggunakan API Pengesanan Emosi, kita boleh melatih model pembelajaran mesin untuk mengenali emosi dalam teks. Dalam bahasa pengaturcaraan Python, ini boleh dicapai dengan menggunakan perpustakaan Natural Language Processing (NLP) seperti NLTK dan Stanford NER. Berikut adalah contoh bagaimana untuk memproses data sentimen menggunakan algoritma NLTK: ```python import nltk from nltk.sentiment import SentimentIntensityAnalyzer # muat turun leksikon yang diperlukan untuk analisis sentimen nltk.download('vader_lexicon') sid = SentimentIntensityAnalyzer() # annotate rentetan \"Saya suka produk ini\" data_sentimen = sid.polarity_scores(\"Saya suka produk ini\") # output: {'neg': 0.0, 'neu': 0.5, 'pos': 0.5, 'compound': 1.0} ``` Untuk menambahkan keupayaan manusia dalam proses ini, kita boleh melatih model pembelajaran mesin menggunakan senarai sentimen yang diberikan oleh manusia. Setelah model dilatih, ia boleh meramalkan sentimen teks yang diberikan oleh pengguna. Sebagai contoh, jika pengguna ingin memberikan ulasan mengenai produk, kita boleh mencadangkan sentimen alternatif dan memberi mereka skor keyakinan untuk setiap pilihan. Cadangan ini akan dipacu oleh sentimen analisis serta pandangan peribadi manusia mengenai teks input. Dengan cara ini, sistem cadangan boleh menjadi lebih komprehensif dan tepat.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(o[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb11aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
